## 第 9 章 事件模块

在上文中提到，Nginx 是一个事件驱动架构的 Web 服务器，本章将全面探讨 Nginx 的事件 驱动机制是如何工作的。ngx_event_t 事件和 ngx_connection_t 连接是处理 TCP 连接的基础数据 结构，在对它们有了基本了解后，在 9.4 节将首先探讨核心模块 ngx_events_module，它定义了 一种新的模块类型—事件模块，而在 9.5 节将开始说明第 1 个事件模块 ngx_event_core_module，它的职责更多地体现在如何管理当前正在使用的事件驱动模式。例 如，在 Nginx 启动时决定到底是基于 select 还是 epoll 来监控网络事件。

epoll 是目前 Linux 操作系统上最强大的事件管理机制，本书描述的场景都是使用 epoll 来 驱动事件的处理，在 9.6 节中，首先会深入到 Linux 内核，研究 epoll 的实现原理和使用方法， 以此明确 epoll 的高并发是怎么来的，以及怎样使用 epoll 才能发挥它的最大性能。接着，就到 了 ngx_epoll_module 模块“亮相”的时候了，这里可以看到一个实际的事件驱动模块是如何实 现声明过的事件抽象接口的（见 9.1 节），同时这个模块也是高效使用 epoll 的较好的例子。

例如，在使用 epoll 时，非常容易遇到过期事件的处理问题，Nginx 就使用了一个巧妙的、成 本低廉的方法完美地解决了这个问题，稍后读者将会看到这个小技巧。

Nginx 的定时器事件是由第 7 章中谈到的红黑树实现的，它也由 epoll 等事件模块触发，在 9.7 节中，读者将看到 Nginx 如何实现独立的定时器功能。

在 9.8 节中，我们开始综合性地介绍事件处理框架，这里将会使用到 9.1 节~9.7 节中的所 有知识。这一节将说明核心的 ngx_process_events_and_timers 方法处理网络事件、定时器事 件、post 事件的完整流程，同时读者会看到 Nginx 是如何解决多个 worker 子进程监听同一端口 引起的“惊群”现象的，以及如何均衡多个 worker 子进程上处理的连接数。

毫无疑问，Linux 内核提供的文件异步 I/O 是不同于 glibc 库实现的多线程伪异步 I/O 的，它 充分利用了在 Linux 内核中 CPU 与 I/O 设备独立工作的特性，使得进程在提交文件异步 I/O 操作 后可以占用 CPU 做其他工作。在 9.9 节中，将会讨论这种高效读取磁盘的机制，在简单说明它 的使用方式后，读者还可以看到文件异步 I/O 是如何集成到 ngx_epoll_module 模块中与 epoll 一 起工作的。

### 9.1 事件处理框架概述

事件处理框架所要解决的问题是如何收集、管理、分发事件。这里所说的事件，主要以 网络事件和定时器事件为主，而网络事件中又以 TCP 网络事件为主（Nginx 毕竟是个 Web 服务 器），本章所述的事件处理框架都将围绕这两种事件进行。

定时器事件将在 9.7 节中阐述，因为它的实现简单而且独立，同时它基于网络事件的触 发实现，并不涉及操作系统内核。这里先来了解一下 Nginx 是如何收集、管理 TCP 网络事件 的。由于网络事件与网卡中断处理程序、内核提供的系统调用密切相关，所以网络事件的驱 动既取决于不同的操作系统平台，在同一个操作系统中也受制于不同的操作系统内核版本。

这样的话，Nginx 支持多少种操作系统（包括支持哪些版本），就必须提供多少个事件驱动 机制，因为基本上每个操作系统提供的事件驱动机制（通常事件驱动机制还有个名字，叫做 I/O 多路复用）都是不同的。例如，Linux 内核 2.6 之前的版本或者大部分类 UNIX 操作系统都可 以使用 poll（ngx_poll_module 模块实现）或者 select（ngx_select_module 模块实现），而 Linux 内核 2.6 之后的版本可以使用 epoll（ngx_epoll_module 模块实现），FreeBSD 上可以使用 kqueue（ngx_kqueue_module 模块实现），Solaris 10 上可以使用 eventport（ngx_eventport_module 模块实现）等。

如此一来，事件处理框架需要在不同的操作系统内核中选择一种事件驱动机制支持网络 事件的处理（Nginx 的高可移植性亦来源于此）。Nginx 是如何做到这一点的呢？ 首先，它定义了一个核心模块 ngx_events_module，这样在 Nginx 启动时会调用 ngx_init_cycle 方法解析配置项，一旦在 nginx.conf 配置文件中找到 ngx_events_module 感兴趣 的“events{}”配置项，ngx_events_module 模块就开始工作了。在图 9-3 中，ngx_events_module 模块定义了事件类型的模块，它的全部工作就是为所有的事件模块解析“events{}”中的配置 项，同时管理这些事件模块存储配置项的结构体。 其次，Nginx 定义了一个非常重要的事件模块 ngx_event_core_module，这个模块会决定使 用哪种事件驱动机制，以及如何管理事件。在 9.5 节中，将会详细讨论 ngx_event_core_module 模块在启动过程中的工作，而在 9.8 节中，则会在事件框架的正常运行中再次看到 ngx_event_core_module 模块的“身影”。

最后，Nginx 定义了一系列（目前为 9 个）运行在不同操作系统、不同内核版本上的事件 驱动模块，包括：ngx_epoll_module、ngx_kqueue_module、ngx_poll_module、 ngx_select_module、ngx_devpoll_module、ngx_eventport_module、ngx_aio_module、 ngx_rtsig_module 和基于 Windows 的 ngx_select_module 模块。在 ngx_event_core_module 模块的初 始化过程中，将会从以上 9 个模块中选取 1 个作为 Nginx 进程的事件驱动模块。

下面开始介绍事件驱动模块接口的相关知识。

事件模块是一种新的模块类型，ngx_module_t 表示 Nginx 模块的基本接口，而针对于每一 种不同类型的模块，都有一个结构体来描述这一类模块的通用接口，这个接口保存在 ngx_module_t 结构体的 ctx 成员中。例如，核心模块的通用接口是 ngx_core_module_t 结构体， 而事件模块的通用接口则是 ngx_event_module_t 结构体（参见图 8-1），具体如下所示。

typedef struct {
// 事件模块的名称
ngx*str_t \_name;
// create_conf 和
init_conf 方法的调用可参见图
9-3
// 在解析配置项前，这个回调方法用于创建存储配置项参数的结构体
void *(*create_conf)(ngx_cycle_t *cycle); /_在解析配置项完成后，
init_conf 方法会被调用，用以综合处理当前事件模块感兴趣的全部配置项
_/
char *(*init_conf)(ngx_cycle_t cycle, void conf); // 对于事件驱动机制，每个事件模块需要实现的
10 个抽象方法
ngx_event_actions_t actions;
} ngx_event_module_t;
ngx_event_module_t 中的 actions 成员是定义事件驱动模块的核心方法，下面重点看一下
actions 中的这 10 个抽象方法，代码如下。

typedef struct {
/_添加事件方法，它将负责把
1 个感兴趣的事件添加到操作系统提供的事件驱动机制（如
epoll、
kqueue 等）中，这样，在事件发生后，将可以在调用下面的
process_events 时获取这个事件
_/
ngx*int_t (*add)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags); /*删除事件方法，它将把
1 个已经存在于事件驱动机制中的事件移除，这样以后即使这个事件发生，调用
process*events 方法时也无法再获取这个事件
*/
ngx*int_t (*del)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags); /*启用
1 个事件，目前事件框架不会调用这个方法，大部分事件驱动模块对于该方法的实现都是与上面的
add 方法完全一致的
*/
ngx_int_t (*enable)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags); /*禁用
1 个事件，目前事件框架不会调用这个方法，大部分事件驱动模块对于该方法的实现都是与上面的
del 方法完全一致的
*/
ngx_int_t (*disable)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags); /*向事件驱动机制中添加一个新的连接，这意味着连接上的读写事件都添加到事件驱动机制中了
*/
ngx_int_t (*add_conn)(ngx_connection_t *c); // 从事件驱动机制中移除一个连接的读写事件
ngx_int_t (*del_conn)(ngx_connection_t *c, ngx_uint_t flags); /*仅在多线程环境下会被调用。目前，
Nginx 在产品环境下还不会以多线程方式运行，因此这里不做讨论
*/
ngx_int_t (*process_changes) (ngx_cycle_t *cycle, ngx_uint_t nowait); /*在正常的工作循环中，将通过调用
process*events 方法来处理事件。这个方法仅在第
8 章中提到的
ngx_process_events_and_timers 方法中调用，它是处理、分发事件的核心
*/
ngx_int_t (*process_events) (ngx_cycle_t *cycle, ngx_msec_t timer, ngx_uint_t flags); // 初始化事件驱动模块的方法
ngx_int_t (*init)(ngx_cycle_t *cycle, ngx_msec_t timer); // 退出事件驱动模块前调用的方法
void (*done)(ngx_cycle_t *cycle);
} ngx_event_actions_t;
9.2 Nginx 事件的定义
在 Nginx 中，每一个事件都由 ngx_event_t 结构体来表示。本节说明 ngx_event_t 中每一个成
员的含义，如下所示。

typedef struct ngx*event_s ngx_event_t; struct ngx_event_s {
/*事件相关的对象。通常
data 都是指向
ngx*connection_t 连接对象。开启文件异步
I/O 时，它可能会指向
ngx_event_aio_t 结构体
*/
void *data;
/*标志位，为
1 时表示事件是可写的。通常情况下，它表示对应的
TCP 连接目前状态是可写的，也就是连接处于可以发送网络包的状态
*/
unsigned write:1;
/*标志位，为
1 时表示为此事件可以建立新的连接。通常情况下，在
ngx*cycle_t 中的
listening 动态数组中，每一个监听对象
ngx_listening_t 对应的读事件中的
accept 标志位才会是
1*/
unsigned accept:1;
/_这个标志位用于区分当前事件是否是过期的，它仅仅是给事件驱动模块使用的，而事件消费模块可不用关心。为什么需要这个标志位呢？当开始处理一批事件时，处理前面的事件可能会关闭一些连接，而这些连接有可能影响这批事件中还未处理到的后面的事件。这时，可通过
instance 标志位来避免处理后面的已经过期的事件。在
9.6 节中，将详细描述
ngx_epoll_module 是如何使用
instance 标志位区分过期事件的，这是一个巧妙的设计方法
_/
unsigned instance:1;
/_标志位，为
1 时表示当前事件是活跃的，为
0 时表示事件是不活跃的。这个状态对应着事件驱动模块处理方式的不同。例如，在添加事件、删除事件和处理事件时，
active 标志位的不同都会对应着不同的处理方式。在使用事件时，一般不会直接改变
active 标志位
_/
unsigned active:1;
/_标志位，为
1 时表示禁用事件，仅在
kqueue 或者
rtsig 事件驱动模块中有效，而对于
epoll 事件驱动模块则无意义，这里不再详述
_/
unsigned disabled:1;
/_标志位，为
1 时表示当前事件已经准备就绪，也就是说，允许这个事件的消费模块处理这个事件。在
HTTP 框架中，经常会检查事件的
ready 标志位以确定是否可以接收请求或者发送响应
_/
unsigned ready:1;
/_该标志位仅对
kqueue，
eventport 等模块有意义，而对于
Linux 上的
epoll 事件驱动模块则是无意义的，限于篇幅，不再详细说明
_/
unsigned oneshot:1;
// 该标志位用于异步
AIO 事件的处理，在
9.9 节中会详细描述
unsigned complete:1;
// 标志位，为
1 时表示当前处理的字符流已经结束
unsigned eof:1;
// 标志位，为
1 时表示事件在处理过程中出现错误
unsigned error:1;
/_标志位，为
1 时表示这个事件已经超时，用以提示事件的消费模块做超时处理，它与
timer_set 都用于
9.7 节将要介绍的定时器
_/
unsigned timedout:1;
// 标志位，为
1 时表示这个事件存在于定时器中
unsigned timer*set:1;
// 标志位，
delayed 为
1 时表示需要延迟处理这个事件，它仅用于限速功能
unsigned delayed:1;
// 该标志位目前没有使用
unsigned read_discarded:1;
// 标志位，目前这个标志位未被使用
unsigned unexpected_eof:1;
/*标志位，为
1 时表示延迟建立
TCP 连接，也就是说，经过
TCP 三次握手后并不建立连接，而是要等到真正收到数据包后才会建立
TCP 连接
*/
unsigned deferred_accept:1;
/*标志位，为
1 时表示等待字符流结束，它只与
kqueue 和
aio 事件驱动机制有关，不再详述
*/
unsigned pending_eof:1;
if !(NGX_THREADS)
// 标志位，如果为
1，则表示在处理
post 事件时，当前事件已经准备就绪
unsigned posted_ready:1;
endif
/*标志位，在
epoll 事件驱动机制下表示一次尽可能多地建立
TCP 连接，它与
multi*accept 配置项对应，实现原理参见
9.8.1 节
*/
unsigned available:1;
// 这个事件发生时的处理方法，每个事件消费模块都会重新实现它
ngx*event_handler_pt handler;
if (NGX_HAVE_AIO)
if (NGX_HAVE_IOCP)
// Windows 系统下的一种事件驱动模型，这里不再详述
ngx_event_ovlp_t ovlp;
else
// Linux aio 机制中定义的结构体，在
9.9 节中会详细说明它
struct aiocb aiocb;
endif
endif
// 由于
epoll 事件驱动方式不使用
index，所以这里不再说明
ngx_uint_t index;
// 可用于记录
error_log 日志的
ngx_log_t 对象
ngx_log_t \_log;
// 定时器节点，用于定时器红黑树中，在
9.7 节会详细介绍
ngx_rbtree_node_t timer;
// 标志位，为
1 时表示当前事件已经关闭，
epoll 模块没有使用它
unsigned closed:1;
// 该标志位目前无实际意义
unsigned channel:1;
// 该标志位目前无实际意义
unsigned resolver:1;
/\_post 事件将会构成一个队列再统一处理，这个队列以
next 和
prev 作为链表指针，以此构成一个简易的双向链表，其中
next 指向后一个事件的地址，
prev 指向前一个事件的地址
*/
ngx_event_t \*next;
ngx_event_t \*\*prev;
};
每一个事件最核心的部分是 handler 回调方法，它将由每一个事件消费模块实现，以此决定这个事件究竟如何“消费”。下面来看一下 handler 方法的原型，代码如下。

typedef void (*ngx_event_handler_pt)(ngx_event_t *ev);
所有的 Nginx 模块只要处理事件就必然要设置 handler 回调方法，后续章节会有许多 handler 回调方法的例子，这里不再详述。

下面开始说明操作事件的方法。

事件是不需要创建的，因为 Nginx 在启动时已经在 ngx_cycle_t 的 read_events 成员中预分配了所有的读事件，并在 write_events 成员中预分配了所有的写事件。事实上，从图 9-1 中我们会看到每一个连接将自动地对应一个写事件和读事件，只要从连接池中获取一个空闲连接就可以拿到事件了。那么，怎么把事件添加到 epoll 等事件驱动模块中呢？需要调用 9.1.1 节中提到的 ngx_event_actions_t 结构体的 add 方法或者 del 方法吗？答案是 Nginx 为我们封装了两个简单的方法用于在事件驱动模块中添加或者移除事件，当然，也可以调用 ngx_event_actions_t 结构体的 add 或者 del 等方法，但并不推荐这样做，因为 Nginx 提供的 ngx_handle_read_event 和 ngx_handle_write_event 方法还是做了许多通用性的工作的。

先看一下 ngx_handle_read_event 方法的原型：
ngx_int_t ngx_handle_read_event(ngx_event_t \*rev, ngx_uint_t flags);
ngx_handle_read_event 方法会将读事件添加到事件驱动模块中，这样该事件对应的 TCP 连接上一旦出现可读事件（如接收到 TCP 连接另一端发送来的字符流）就会回调该事件的 handler 方法。

下面看一下 ngx_handle_read_event 的参数和返回值。参数 rev 是要操作的事件，flags 将会指定事件的驱动方式。对于不同的事件驱动模块，flags 的取值范围并不同，本书以 Linux 下的 epoll 为例，对于 ngx_epoll_module 来说，flags 的取值范围可以是 0 或者 NGX_CLOSE_EVENT（NGX_CLOSE_EVENT 仅在 epoll 的 LT 水平触发模式下有效），Nginx 主要工作在 ET 模式下，一般可以忽略 flags 这个参数。该方法返回 NGX_OK 表示成功，返回 NGX_ERROR 表示失败。

再看一下 ngx_handle_write_event 方法的原型：
ngx_int_t ngx_handle_write_event(ngx_event_t \*wev, size_t lowat);
ngx_handle_write_event 方法会将写事件添加到事件驱动模块中。wev 是要操作的事件，而 lowat 则表示只有当连接对应的套接字缓冲区中必须有 lowat 大小的可用空间时，事件收集器（如 select 或者 epoll_wait 调用）才能处理这个可写事件（lowat 参数为 0 时表示不考虑可写缓冲区的大小）。该方法返回 NGX_OK 表示成功，返回 NGX_ERROR 表示失败。

一般在向 epoll 中添加可读或者可写事件时，都是使用 ngx_handle_read_event 或者 ngx_handle_write_event 方法的。对于事件驱动模块实现的 ngx_event_actions 结构体中的事件设置方法，最好不要直接调用，下面这 4 个方法直接使用时都会与具体的事件驱动机制强相关，而使用 ngx_handle_read_event 或者 ngx_handle_write_event 方法则可以屏蔽这种差异。

define ngx_add_event ngx_event_actions.add #define ngx_del_event ngx_event_actions.del #define ngx_add_conn ngx_event_actions.add_conn #define ngx_del_conn ngx_event_actions.del_conn

### 9.3 Nginx 连接的定义

作为 Web 服务器，每一个用户请求至少对应着一个 TCP 连接，为了及时处理这个连接， 至少需要一个读事件和一个写事件，使得 epoll 可以有效地根据触发的事件调度相应模块读取请求或者发送响应。因此，Nginx 中定义了基本的数据结构 ngx_connection_t 来表示连接，这个连接表示是客户端主动发起的、Nginx 服务器被动接受的 TCP 连接，我们可以简单称其为被动连接。同时，在有些请求的处理过程中，Nginx 会试图主动向其他上游服务器建立连接，并以此连接与上游服务器通信，因此，这样的连接与 ngx_connection_t 又是不同的，Nginx 定义了 ngx_peer_connection_t 结构体来表示主动连接，当然，ngx_peer_connection_t 主动连接是以 ngx_connection_t 结构体为基础实现的。本节将说明这两种连接中各字段的意义，同时需要注意的是，这两种连接都不可以随意创建，必须从连接池中获取，在 9.3.3 节中会说明连接池的用法。

#### 9.3.1 被动连接

本章中未加修饰提到的“连接”都是指客户端发起的、服务器被动接受的连接，这样的连接都是使用 ngx_connection_t 结构体表示的，其定义如下。

```C
typedef struct ngx_connection_s ngx_connection_t;

struct ngx_connection_s {
    // 连接未使用时，data 成员用于充当连接池中空闲连接链表中的 next 指针。当连接被使用时，data 的意义由使用它的 Nginx 模块而定，如在 HTTP 框架中，data 指向 ngx_http_request_t 请求
    void *data;
    
    // 连接对应的读事件
    ngx_event_t *read;
    
    // 连接对应的写事件
    ngx_event_t *write;
    
    // 套接字句柄
    ngx_socket_t fd;
    
    // 直接接收网络字符流的方法
    ngx_recv_pt recv;
    
    // 直接发送网络字符流的方法
    ngx_send_pt send;
    
    // 以 ngx_chain_t 链表为参数来接收网络字符流的方法
    ngx_recv_chain_pt recv_chain;
    
    // 以 ngx_chain_t 链表为参数来发送网络字符流的方法
    ngx_send_chain_pt send_chain;
    
    // 这个连接对应的 ngx_listening_t 监听对象，此连接由 listening 监听端口的事件建立
    ngx_listening_t *listening;
    
    // 这个连接上已经发送出去的字节数
    off_t sent;
    
    // 可以记录日志的 ngx_log_t 对象
    ngx_log_t *log;
    
    // 内存池。一般在 accept 一个新连接时，会创建一个内存池，而在这个连接结束时会销毁内存池。注意，这里所说的连接是指成功建立的 TCP 连接，所有的 ngx_connection_t 结构体都是预分配的。这个内存池的大小将由上面的 listening 监听对象中的 pool_size 成员决定
    ngx_pool_t *pool;
    
    // 连接客户端的 sockaddr 结构体
    struct sockaddr *sockaddr;
    
    // sockaddr 结构体的长度
    socklen_t socklen;
    
    // 连接客户端字符串形式的 IP 地址
    ngx_str_t addr_text;
    
    // 本机的监听端口对应的 sockaddr 结构体，也就是 listening 监听对象中的 sockaddr 成员
    struct sockaddr *local_sockaddr;
    
    // 用于接收、缓存客户端发来的字符流，每个事件消费模块可自由决定从连接池中分配多大的空间给 buffer 这个接收缓存字段。例如，在 HTTP 模块中，它的大小决定于 client_header_buffer_size 配置项
    ngx_buf_t *buffer;
    
    // 该字段用来将当前连接以双向链表元素的形式添加到 ngx_cycle_t 核心结构体的 reusable_connections_queue 双向链表中，表示可以重用的连接
    ngx_queue_t queue;
    
    // 连接使用次数。ngx_connection_t 结构体每次建立一条来自客户端的连接，或者用于主动向后端服务器发起连接时（ngx_peer_connection_t 也使用它），number 都会加 1
    ngx_atomic_uint_t number;
    
    // 处理的请求次数
    ngx_uint_t requests;
    
    // 缓存中的业务类型。任何事件消费模块都可以自定义需要的标志位。这个 buffered 字段有 8 位，最多可以同时表示 8 个不同的业务。
    unsigned buffered:8;
    
    // 本连接记录日志时的级别，它占用了 3 位，取值范围是 0~7，但实际上目前只定义了 5 个值，由 ngx_connection_log_error_e 枚举表示
    unsigned log_error:3;
    
    // 标志位，为 1 时表示独立的连接，如从客户端发起的连接；为 0 时表示依靠其他连接的行为而建立起来的非独立连接，如使用 upstream 机制向后端服务器建立起来的连接
    unsigned single_connection:1;
    
    // 标志位，为 1 时表示不期待字符流结束，目前无意义
    unsigned unexpected_eof:1;
    
    // 标志位，为 1 时表示连接已经超时
    unsigned timedout:1;
    
    // 标志位，为 1 时表示连接处理过程中出现错误
    unsigned error:1;
    
    // 标志位，为 1 时表示连接已经销毁。这里的连接指是的 TCP 连接，而不是 ngx_connection_t 结构体。当 destroyed 为 1 时，ngx_connection_t 结构体仍然存在，但其对应的套接字、内存池等已经不可用
    unsigned destroyed:1;
    
    // 标志位，为 1 时表示连接处于空闲状态，如 keepalive 请求中两次请求之间的状态
    unsigned idle:1;
    
    // 标志位，为 1 时表示连接可重用，它与上面的 queue 字段是对应使用的
    unsigned reusable:1;
    
    // 标志位，为 1 时表示连接关闭
    unsigned close:1;
    
    // 标志位，为 1 时表示正在将文件中的数据发往连接的另一端
    unsigned sendfile:1;
    
    // 标志位，如果为 1，则表示只有在连接套接字对应的发送缓冲区必须满足最低设置的大小阈值时，事件驱动模块才会分发该事件
    unsigned sndlowat:1;
    
    // 标志位，表示如何使用 TCP 的 nodelay 特性
    unsigned tcp_nodelay:2;
    
    // 标志位，表示如何使用 TCP 的 nopush 特性
    unsigned tcp_nopush:2;

#if (NGX_HAVE_AIO_SENDFILE)
    // 标志位，为 1 时表示使用异步 I/O 的方式将磁盘上文件发送给网络连接的另一端
    unsigned aio_sendfile:1;
    
    // 使用异步 I/O 方式发送的文件，busy_sendfile 缓冲区保存待发送文件的信息
    ngx_buf_t *busy_sendfile;
#endif
};
```
链表中的 recv、send、recv_chain、send_chain 这 4 个关于接收、发送网络字符流的方法原型定义如下。

typedef ssize_t (*ngx_recv_pt)(ngx_connection_t c, u_char buf, size_t size); 
typedef ssize_t (*ngx_recv_chain_pt)(ngx_connection_t c, ngx_chain_t in); 
typedef ssize_t (\*ngx_send_pt)(ngx_connection_t c
这 4 个成员以方法指针的形式出现，说明每个连接都可以采用不同的接收方法，每个事件消费模块都可以灵活地决定其行为。不同的事件驱动机制需要使用的接收、发送方法多半是不一样的，在 9.6 节中，读者可以看到 ngx_epoll_module 模块是如何定义这 4 种方法的。

#### 9.3.2 主动连接

作为 Web 服务器，Nginx 也需要向其他服务器主动发起连接，当然，这样的连接与上一节介绍的被动连接是不同的，它使用 ngx_peer_connection_t 结构体来表示主动连接。不过，一个待处理连接的许多特性在被动连接结构体 ngx_connection_t 中都定义过了，因此，在 ngx_peer_connection_t 结构体中引用了 ngx_connection_t 这个结构体，下面我们来看一下其定义。

```c
typedef struct ngx_peer_connection_s ngx_peer_connection_t;

// 当使用长连接与上游服务器通信时，可通过该方法由连接池中获取一个新连接
typedef ngx_int_t (*ngx_event_get_peer_pt)(ngx_peer_connection_t *pc, void *data);

// 当使用长连接与上游服务器通信时，通过该方法将使用完毕的连接释放给连接池
typedef void (*ngx_event_free_peer_pt)(ngx_peer_connection_t *pc, void *data, ngx_uint_t state);

struct ngx_peer_connection_s {
    // 一个主动连接实际上也需要 ngx_connection_t 结构体中的大部分成员，并且出于重用的考虑而定义了 connection 成员
    ngx_connection_t *connection;
    
    // 远端服务器的 socket 地址
    struct sockaddr *sockaddr;
    
    // sockaddr 地址的长度
    socklen_t socklen;
    
    // 远端服务器的名称
    ngx_str_t *name;
    
    // 表示在连接一个远端服务器时，当前连接出现异常失败后可以重试的次数，也就是允许的最多失败次数
    ngx_uint_t tries;
    
    // 获取连接的方法，如果使用长连接构成的连接池，那么必须要实现 get 方法
    ngx_event_get_peer_pt get;
    
    // 与 get 方法对应的释放连接的方法
    ngx_event_free_peer_pt free;
    
    // 这个 data 指针仅用于和上面的 get、free 方法配合传递参数，它的具体含义与实现 get 方法、free 方法的模块相关，可参照 ngx_event_get_peer_pt 和 ngx_event_free_peer_pt 方法原型中的 data 参数
    void *data;
    
    // 本机地址信息
    ngx_addr_t *local;
    
    // 套接字的接收缓冲区大小
    int rcvbuf;
    
    // 记录日志的 ngx_log_t 对象
    ngx_log_t *log;
    
    // 标志位，为 1 时表示上面的 connection 连接已经缓存
    unsigned cached:1;
    
    // 与 ngx_connection_t 里的 log_error 意义是相同的，区别在于这里的 log_error 只有两位，只能表达 4 种错误，NGX_ERROR_IGNORE_EINVAL 错误无法表达
    unsigned log_error:2;
};

```
ngx_peer_connection_t 也有一个 ngx_connection_t 类型的成员，怎么理解这两个结构体之间的关系呢？所有的事件消费模块在每次使用 ngx_peer_connection_t 对象时，一般都需要重新生成一个 ngx_peer_connection_t 结构体，然而，ngx_peer_connection_t 对应的 ngx_connection_t 连接一般还是从连接池中获取，因此，ngx_peer_connection_t 只是对 ngx_connection_t 结构体做了简单的包装而已。

9.3.3 ngx_connection_t 连接池
Nginx 在接受客户端的连接时，所使用的 ngx_connection_t 结构体都是在启动阶段就预分配好的，使用时从连接池中获取即可。这个连接池是如何封装的呢？如图 9-1 所示。

从图 9-1 中可以看出，在 ngx_cycle_t 中的 connections 和 free_connections 这两个成员构成了一个连接池，其中 connections 指向整个连接池数组的首部，而 free_connections 则指向第一个 ngx_connection_t 空闲连接。所有的空闲连接 ngx_connection_t 都以 data 成员（见 9.3.1 节）作为 next 指针串联成一个单链表，如此，一旦有用户发起连接时就从 free_connections 指向的链表头获取一个空闲的连接，同时 free_connections 再指向下一个空闲连接。而归还连接时只需把该连接插入到 free_connections 链表表头即可。

图 9-1 中还显示了事件池，Nginx 认为每一个连接一定至少需要一个读事件和一个写事件，有多少连接就分配多少个读、写事件。怎样把连接池中的任一个连接与读事件、写事件对应起来呢？很简单。由于读事件、写事件、连接池是由 3 个大小相同的数组组成，所以根据数组序号就可将每一个连接、读事件、写事件对应起来，这个对应关系在 ngx_event_core_module 模块的初始化过程中就已经决定了（参见 9.5 节）。这 3 个数组的大小都是由 nginx.conf 中的 connections 配置项决定的。

在使用连接池时，Nginx 也封装了两个方法，见表 9-1。

如果我们开发的模块直接使用了连接池，那么就可以用这两个方法来获取、释放 ngx_connection_t 结构体。

图 9-1 ngx_connection_t 连接池示意图
表 9-1 连接池的使用方法
9.4 ngx_events_module 核心模块
ngx_events_module 模块是一个核心模块，它定义了一类新模块：事件模块。它的功能如
下：定义新的事件类型，并定义每个事件模块都需要实现的 ngx_event_module_t 接口（参见
9.1.1 节），还需要管理这些事件模块生成的配置项结构体，并解析事件类配置项，当然，在
解析配置项时会调用其在 ngx_command_t 数组中定义的回调方法。这些过程在下文中都会介
绍，不过，首先还是看一下 ngx_events_module 模块的定义。

就像在第 3 章中我们曾经做过的一样，定义一个 Nginx 模块就是在实现 ngx_modult_t 结构
体。这里需要先定义好 ngx_command_t（决定这个模块如何处理自己感兴趣的配置项）数
组，因为任何模块都是以配置项来定制功能的。ngx_events_commands 数组决定了
ngx_events_module 模块是如何定制其功能的，代码如下。

static ngx_command_t ngx_events_commands[] = {
{ ngx_string("events"),
NGX_MAIN_CONF|NGX_CONF_BLOCK|NGX_CONF_NOARGS,
ngx_events_block,
0,
0,
NULL },
ngx_null_command
};
可以看到，ngx_events_module 模块只对一个块配置项感兴趣，也就是 nginx.conf 中必须有
的 events{...}配置项。注意，这里暂时先不要关心 ngx_events_block 方法是如何处理这个配置
项的。

作为核心模块，ngx_events_module 还需要实现核心模块的共同接口 ngx_core_module_t，
如下所示。

static ngx_core_module_t ngx_events_module_ctx = {
ngx_string("events"),
NULL,
NULL
};
可以看到，ngx_events_module_ctx 实现的接口只是定义了模块名字而已，
ngx_core_module_t 接口中定义的 create_conf 方法和 init_conf 方法都没有实现（NULL 空指针即
为不实现），为什么呢？这是因为 ngx_events_module 模块并不会解析配置项的参数，只是在
出现 events 配置项后会调用各事件模块去解析 events{...}块内的配置项，自然就不需要实现
create_conf 方法来创建存储配置项参数的结构体，也不需要实现 init_conf 方法处理解析出的配
置项。

最后看一下 ngx_events_module 模块的定义代码如下。

```C
ngx_module_t ngx_events_module = {
NGX_MODULE_V1,
&ngx_events_module_ctx, /* module context
ngx_events_commands, module directives
NGX_CORE_MODULE, module type
NULL, init master
NULL, init module
NULL, init process
NULL, init thread
NULL, exit thread
NULL, exit process
NULL, exit master */
NGX_MODULE_V1_PADDING
};
```

可见，除了对 events 配置项的解析外，该模块没有做其他任何事情。下面开始介绍在解
析 events 配置块时，ngx_events_block 方法做了些什么。

9.4.1 如何管理所有事件模块的配置项
上文说过，每一个事件模块都必须实现 ngx_event_module_t 接口，这个接口中允许每个事
件模块建立自己的配置项结构体，用于存储感兴趣的配置项在 nginx.conf 中对应的参数。

ngx_event_module_t 中的 create_conf 方法就是用于创建这个结构体的方法，事件模块只需要在
这个方法中分配内存即可，但这个内存指针是如何由 ngx_events_module 模块管理的呢？下面
来看一下这些事件模块的配置项指针是如何被存放的，如图 9-2 所示。

每一个事件模块产生的配置结构体指针都会被放到 ngx\*events_module 模块创建的指针数
组中，可这个指针数组又存放到哪里呢？看一下 ngx_cycle_t 核心结构体中的 conf_ctx 成员，它
指向一个指针数组，而这个指针数组中就依次存放着所有的 Nginx 模块关于配置项方面的指
针。在默认的编译顺序下，从 ngx_modules.c 文件中可以看到 ngx_events_module 模块是在
ngx_modules 数组中的第 4 个位置，因此，所有进程的 conf_ctx 数组的第 4 个指针就保存着上面
说过的 ngx_events_module 模块创建的指针数组。解释是不是有点绕？再回顾一下 ngx_cycle_t
结构体中的 conf_ctx 的定义：
void \**\*\*conf*ctx;
为什么上面代码中有 4 个\*？因为它首先指向一个存放指针的数组，这个数组中的指针成
员同时又指向了另外一个存放指针的数组，所以是 4 个\*。看到 conf_ctx 的奥秘了吧。只有拥
有了这个 conf_ctx，才可以看到任意一个模块在 create_conf 中产生的结构体指针。同理，
HTTP 模块和 mail 模块也是这样做的，这些模块的通用接口中也有 create_conf 方法，其产生的
指针会以相似的方式存放。

每一个事件模块如何获取它在 create_conf 中分配的结构体的指针呢？ngx_events_module
定义了一个简单的宏来完成这个功能代码，如下。

define ngx_event_get_conf(conf_ctx,module) \
(\*(ngx_get_conf(conf_ctx, ngx_events_module))) [module.ctx_index];
ngx_get_conf 也是一个宏，它用来获取图 9-1 中第一个数组中的指针，如下所示。

define ngx_get_conf(conf_ctx, module) conf_ctx[module.index]
因此，调用 ngx_event_get_conf 时只需要在第 1 个参数中传入 ngx_cycle_t 中的 conf_ctx 成
员，在第 2 个参数中传入自己的模块名，就可以获取配置项结构体的指针。详细内容可参见
9.6.3 节中使用 ngx_epoll_module 的 ngx_epoll_init 方法获取配置项的例子。

图 9-2 所有事件模块配置项结构体的指针是如何管理的
9.4.2 管理事件模块
上文说到，配置项结构体指针的保存都是在 ngx_events_block 方法中进行的。下面再来看
一下这个方法执行的流程图，如图 9-3 所示。

图 9-3 ngx_events_module 核心模块如何加载事件模块
下面简要描述一下这 5 个步骤。

1）首先初始化所有事件模块的 ctx_index 成员。这里要先回顾一下 ngx_module_t 模块接口
的定义，如下所示。

struct ngx_module_s {
ngx_uint_t ctx_index;
ngx_uint_t index;
…
}
这里的 index 是所有模块在 ngx_modules.c 文件的 ngx_modules 数组中的序号，它与
ngx_modules 数组中所有模块的顺序是一致的。什么时候初始化这个 index 呢？启动 Nginx 后，
在调用第 8 章中介绍过的 ngx_init_cycle 方法前就会进行，代码如下。

ngx_max_module = 0;
for (i = 0; ngx_modules[i]; i++) {
ngx_modules[i]-\>index = ngx_max_module++;
}
其中，ngx_max_module 是 Nginx 模块的总个数。注意，本书前文曾多次提到过，Nginx 各
模块在 ngx_modules 数组中的顺序是很重要的，依靠 index 成员，每一个模块才可以把自己的
位置与其他模块的位置进行比较，并以此决定行为。但是，Nginx 同时又允许再次定义子类
型，如事件类型、HTTP 类型、mail 类型，那同一类型的模块间又如何区分顺序呢（依靠
index 当然可以区分顺序，但 index 是针对所有模块的，这样效率太差）？这就得依靠 ctx_index
成员了。ctx_index 表明了模块在相同类型模块中的顺序。

因此，ngx_events_block 方法的第一步就是初始化所有事件模块的 ctx_index 成员，这会决
定以后加载各事件模块的顺序。其代码非常简单，如下所示。

ngx_event_max_module = 0;
for (i = 0; ngx_modules[i]; i++) {
if (ngx_modules[i]-\>type != NGX_EVENT_MODULE) {
continue;
}
ngx_modules[i]-\>ctx_index = ngx_event_max_module++;
}
其中，ngx_event_max_module 是编译进 Nginx 的所有事件模块的总个数。

2）分配 9.4.1 节中介绍的指针数组，不再详述。

3）依次调用所有事件模块通用接口 ngx_event_module_t 中的 create_conf 方法，当然，产
生的结构体的指针保存在上面的指针数组中。

4）针对所有事件类型的模块解析配置项。这时，每个事件模块定义的 ngx_command_t 决
定了配置项的解析方法，如果在 nginx.conf 中发现相应的配置项，就会回调各事件模块定义的
方法。

5）解析完配置项后，依次调用所有事件模块通用接口 ngx_event_module_t 中的 init_conf 方
法，实现了这个方法的事件模块可以在此做一些配置参数的整合工作。

以上就是 ngx_events_module 模块的核心工作流程。对于事件驱动机制，更多的工作是在
ngx_event_core_module 模块中进行的，下面继续看一下这个模块做了些什么。

9.5 ngx_event_core_module 事件模块
ngx_event_core_module 模块是一个事件类型的模块，它在所有事件模块中的顺序是第一
位（configure 执行时必须把它放在其他事件模块之前）。这就保证了它会先于其他事件模块
执行，由此它选择事件驱动机制的任务才可以完成。

ngx_event_core_module 模块要完成哪些任务呢？它会创建 9.3 节中介绍的连接池（包括读/
写事件），同时会决定究竟使用哪些事件驱动机制，以及初始化将要使用的事件模块。

下面先来看一下 ngx_event_core_module 模块对哪些配置项感兴趣。该模块定义了
ngx_event_core_commands 数组处理其感兴趣的 7 个配置项，以下进行简要说明。

static ngx*command_t ngx_event_core_commands[] = {
/*连接池的大小，也就是每个
worker 进程中支持的
TCP 最大连接数，它与下面的
connections 配置项的意义是重复的，可参照
9.3.3 节理解连接池的概念
*/
{ ngx_string("worker_connections"),
NGX_EVENT_CONF|NGX_CONF_TAKE1,
ngx_event_connections,
0,
0,
NULL },
// 连接池的大小，与
worker_connections 配置项意义相同
{ ngx_string("connections"),
NGX_EVENT_CONF|NGX_CONF_TAKE1,
ngx_event_connections,
0,
0,
NULL },
// 确定选择哪一个事件模块作为事件驱动机制
{ ngx_string("use"),
NGX_EVENT_CONF|NGX_CONF_TAKE1,
ngx_event_use,
0,
0,
NULL },
/*对应于
9.2 节中提到的事件定义的
available 字段。对于
epoll 事件驱动模式来说，意味着在接收到一个新连接事件时，调用
accept 以尽可能多地接收连接
*/
{ ngx_string("multi_accept"),
NGX_EVENT_CONF|NGX_CONF_FLAG,
ngx_conf_set_flag_slot,
0,
offsetof(ngx_event_conf_t, multi_accept),
NULL },
// 确定是否使用
accept_mutex 负载均衡锁，默认为开启
{ ngx_string("accept_mutex"),
NGX_EVENT_CONF|NGX_CONF_FLAG,
ngx_conf_set_flag_slot,
0,
offsetof(ngx_event_conf_t, accept_mutex),
NULL },
/*启用
accept*mutex 负载均衡锁后，延迟
accept_mutex_delay 毫秒后再试图处理新连接事件
*/
{ ngx_string("accept_mutex_delay"),
NGX_EVENT_CONF|NGX_CONF_TAKE1,
ngx_conf_set_msec_slot,
0,
offsetof(ngx_event_conf_t, accept_mutex_delay),
NULL },
// 需要对来自指定
IP 的
TCP 连接打印
debug 级别的调试日志
{ ngx_string("debug_connection"),
NGX_EVENT_CONF|NGX_CONF_TAKE1,
ngx_event_debug_connection,
0,
0,
NULL },
ngx_null_command
};
值得注意的是，上面对于配置项参数的解析使用了在第 4 章中介绍过的 Nginx 预设的配置
项解析方法，如 ngx_conf_set_flag_slot 和 ngx_conf_set_msec_slot。这种自动解析配置项的方式
是根据指定结构体中的位置决定的。下面看一下该模块定义的用于存储配置项参数的结构体
ngx_event_conf_t。

pedef struct {
// 连接池的大小
ngx*uint_t connections;
/*选用的事件模块在所有事件模块中的序号，也就是
9.4.2 节中介绍过的
ctx*index 成员
*/
ngx*uint_t use;
// 标志位，如果为
1，则表示在接收到一个新连接事件时，一次性建立尽可能多的连接
ngx_flag_t multi_accept;
// 标志位，为
1 时表示启用负载均衡锁
ngx_flag_t accept_mutex;
/*负载均衡锁会使有些
worker 进程在拿不到锁时延迟建立新连接，
accept*mutex_delay 就是这段延迟时间的长度。关于它如何影响负载均衡的内容，可参见
9.8.5 节
*/
ngx_msec_t accept_mutex_delay;
// 所选用事件模块的名字，它与
use 成员是匹配的
u_char *name;
if (NGX_DEBUG)
/*在—
with-debug 编译模式下，可以仅针对某些客户端建立的连接输出调试级别的日志，而
debug_connection 数组用于保存这些客户端的地址信息
\*/
ngx_array_t debug_connection;
endif
} ngx_event_conf_t;
ngx_event_conf_t 结构体中有两个成员与负载均衡锁相关，读者可以在 9.8 节中了解负载均
衡锁的原理。

对于每个事件模块都需要实现的 ngx_event_module_t 接口，ngx_event_core_module 模块则
仅实现了 create_conf 方法和 init_conf 方法，这是因为它并不真正负责 TCP 网络事件的驱动，所
以不会实现 ngx_event_actions_t 中的方法，如下所示。

static ngx*str_t event_core_name = ngx_string("event_core");
ngx_event_module_t ngx_event_core_module_ctx = {
&event_core_name,
ngx_event_create_conf, /* create configuration
ngx*event_init_conf, init configuration */
{ NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL }
};
最后看一下 ngx_event_core_module 模块的定义。

ngx*module_t ngx_event_core_module = {
NGX_MODULE_V1,
&ngx_event_core_module_ctx, /* module context
ngx*event_core_commands, module directives
NGX_EVENT_MODULE, module type
NULL, init master
ngx_event_module_init, init module
ngx_event_process_init, init process
NULL, init thread
NULL, exit thread
NULL, exit process
NULL, exit master */
NGX_MODULE_V1_PADDING
};
它实现了 ngx_event_module_init 方法和 ngx_event_process_init 方法。在 Nginx 启动过程中还
没有 fork 出 worker 子进程时，会首先调用 ngx_event_core_module 模块的 ngx_event_module_init
方法（参见图 8-6），而在 fork 出 worker 子进程后，每一个 worker 进程会在调用
ngx_event_core_module 模块的 ngx_event_process_init 方法后才会进入正式的工作循环。弄清楚
这两个方法何时调用后，下面来看一下它们究竟做了什么。

ngx_event_module_init 方法其实很简单，它主要初始化了一些变量，尤其是
ngx_http_stub_status_module 统计模块使用的一些原子性的统计变量，这里不再详述。

而 ngx_event_process_init 方法就做了许多事情，下面开始详细介绍它的流程。

ngx_event_core_module 模块在启动过程中的主要工作都是在 ngx_event_process_init 方法中
进行的，如图 9-4 所示。

下面对以上 13 个步骤进行简要说明。

1）当打开 accept_mutex 负载均衡锁，同时使用了 master 模式并且 worker 进程数量大于 1
时，才正式确定了进程将使用 accept_mutex 负载均衡锁。因此，即使我们在配置文件中指定
打开 accept_mutex 锁，如果没有使用 master 模式或者 worker 进程数量等于 1，进程在运行时还
是不会使用负载均衡锁（既然不存在多个进程去抢一个监听端口上的连接的情况，那么自然
不需要均衡多个 worker 进程的负载）。

这时会将 ngx_use_accept_mutex 全局变量置为 1，ngx_accept_mutex_held 标志设为 0，
ngx_accept_mutex_delay 则设为在配置文件中指定的最大延迟时间。这 3 个变量的意义可参见
9.8 节中关于负载均衡锁的说明。

2）如果没有满足第 1 步中的 3 个条件，那么会把 ngx_use_accept_mutex 置为 0，也就是关闭
负载均衡锁。

3）初始化红黑树实现的定时器。关于定时器的实现细节可参见 9.6 节。

4）在调用 use 配置项指定的事件模块中，在 ngx_event_module_t 接口下，
ngx_event_actions_t 中的 init 方法进行这个事件模块的初始化工作。

图 9-4 ngx_event_core_module 事件模块启动时的工作流程
5）如果 nginx.conf 配置文件中设置了 timer_resolution 配置项，即表明需要控制时间精度，
这时会调用 setitimer 方法，设置时间间隔为 timer_resolution 毫秒来回调 ngx_timer_signal_handler
方法。下面简单地介绍一下 Nginx 是如何控制时间精度的。

ngx_timer_signal_handler 方法又做了些什么呢？其实非常简单，如下所示。

void ngx_timer_signal_handler(int signo)
{
ngx_event_timer_alarm = 1;
}
ngx_event_timer_alarm 只是个全局变量，当它设为 1 时，表示需要更新时间。

在 ngx_event_actions_t 的 process_events 方法中，每一个事件驱动模块都需要在
ngx_event_timer_alarm 为 1 时调用 ngx_time_update 方法（参见 9.7.1 节）更新系统时间，在更新
系统结束后需要将 ngx_event_timer_alarm 设为 0。

6）如果使用了 epoll 事件驱动模式，那么会为 ngx_cycle_t 结构体中的 files 成员预分配句
柄。本章仅针对 epoll 事件驱动模式，具体内容不再详述。

7）预分配 ngx_connection_t 数组作为连接池，同时将 ngx_cycle_t 结构体中的 connections 成
员指向该数组。数组的个数为 nginx.conf 配置文件中 connections 或 worker_connections 中配置的
连接数。

8）预分配 ngx_event_t 事件数组作为读事件池，同时将 ngx_cycle_t 结构体中的 read_events
成员指向该数组。数组的个数为 nginx.conf 配置文件中 connections 或 worker_connections 里配置
的连接数。

9）预分配 ngx_event_t 事件数组作为写事件池，同时将 ngx_cycle_t 结构体中的 write_events
成员指向该数组。数组的个数为 nginx.conf 配置文件中 connections 或 worker_connections 里配置
的连接数。

10）按照序号，将上述 3 个数组相应的读/写事件设置到每一个 ngx_connection_t 连接对象
中，同时把这些连接以 ngx_connection_t 中的 data 成员作为 next 指针串联成链表，为下一步设置
空闲连接链表做好准备，参见图 9-1。

11）将 ngx_cycle_t 结构体中的空闲连接链表 free_connections 指向 connections 数组的最后 1
个元素，也就是第 10 步所有 ngx_connection_t 连接通过 data 成员组成的单链表的首部。

12）在刚刚建立好的连接池中，为所有 ngx_listening_t 监听对象中的 connection 成员分配
连接，同时对监听端口的读事件设置处理方法为 ngx_event_accept，也就是说，有新连接事件
时将调用 ngx_event_accept 方法建立新连接（详见 9.8 节中关于如何建立新连接的内容）。

13）将监听对象连接的读事件添加到事件驱动模块中，这样，epoll 等事件模块就开始检
测监听服务，并开始向用户提供服务了。注意，打开 accept_mutex 锁后则不执行这一步。

至此，ngx_event_core_module 模块的启动工作就全部结束了。下面将以 epoll 事件方式为
例来介绍实际的事件驱动模块是如何处理事件的。

9.6 epoll 事件驱动模块
本章 9.1 节~9.5 节都在探讨 Nginx 是如何设计事件驱动框架、如何管理不同的事件驱动模
块的，但本节中将以 epoll 为例，讨论 Linux 操作系统内核是如何实现 epoll 事件驱动机制的，
在简单了解它的用法后，会进一步说明 ngx_epoll_module 模块是如何基于 epoll 实现 Nginx 的事
件驱动的。这样读者就会对 Nginx 完整的事件驱动设计方法有全面的了解，同时可以弄清楚
Nginx 在几十万并发连接下是如何做到高效利用服务器资源的。

9.6.1 epoll 的原理和用法
设想一个场景：有 100 万用户同时与一个进程保持着 TCP 连接，而每一时刻只有几十个
或几百个 TCP 连接是活跃的（接收到 TCP 包），也就是说，在每一时刻，进程只需要处理这
100 万连接中的一小部分连接。那么，如何才能高效地处理这种场景呢？进程是否在每次询
问操作系统收集有事件发生的 TCP 连接时，把这 100 万个连接告诉操作系统，然后由操作系
统找出其中有事件发生的几百个连接呢？实际上，在 Linux 内核 2.4 版本以前，那时的 select 或
者 poll 事件驱动方式就是这样做的。

这里有个非常明显的问题，即在某一时刻，进程收集有事件的连接时，其实这 100 万连
接中的大部分都是没有事件发生的。因此，如果每次收集事件时，都把这 100 万连接的套接
字传给操作系统（这首先就是用户态内存到内核态内存的大量复制），而由操作系统内核寻
找这些连接上有没有未处理的事件，将会是巨大的资源浪费，然而 select 和 poll 就是这样做
的，因此它们最多只能处理几千个并发连接。而 epoll 不这样做，它在 Linux 内核中申请了一
个简易的文件系统，把原先的一个 select 或者 poll 调用分成了 3 个部分：调用 epoll_create 建立 1
个 epoll 对象（在 epoll 文件系统中给这个句柄分配资源）、调用 epoll_ctl 向 epoll 对象中添加这
100 万个连接的套接字、调用 epoll_wait 收集发生事件的连接。这样，只需要在进程启动时建
立 1 个 epoll 对象，并在需要的时候向它添加或删除连接就可以了，因此，在实际收集事件
时，epoll_wait 的效率就会非常高，因为调用 epoll_wait 时并没有向它传递这 100 万个连接，内
核也不需要去遍历全部的连接。

那么，Linux 内核将如何实现以上的想法呢？下面以 Linux 内核 2.6.35 版本为例，简单说明
一下 epoll 是如何高效处理事件的。图 9-5 展示了 epoll 的内部主要数据结构是如何安排的。

当某一个进程调用 epoll_create 方法时，Linux 内核会创建一个 eventpoll 结构体，这个结构
体中有两个成员与 epoll 的使用方式密切相关，如下所示。

struct eventpoll {
…
/_红黑树的根节点，这棵树中存储着所有添加到
epoll 中的事件，也就是这个
epoll 监控的事件
_/
struct rb_root rbr;
// 双向链表
rdllist 保存着将要通过
epoll_wait 返回给用户的、满足条件的事件
struct list_head rdllist;
…
};
图 9-5 epoll 原理示意图
每一个 epoll 对象都有一个独立的 eventpoll 结构体，这个结构体会在内核空间中创造独立
的内存，用于存储使用 epoll_ctl 方法向 epoll 对象中添加进来的事件。这些事件都会挂到 rbr 红
黑树中，这样，重复添加的事件就可以通过红黑树而高效地识别出来（epoll_ctl 方法会很
快）。Linux 内核中的这棵红黑树与第 7 章中介绍的 Nginx 红黑树是非常相似的，可以参照
ngx_rbtree_t 容器进行理解。

所有添加到 epoll 中的事件都会与设备（如网卡）驱动程序建立回调关系，也就是说，相
应的事件发生时会调用这里的回调方法。这个回调方法在内核中叫做 ep_poll_callback，它会
把这样的事件放到上面的 rdllist 双向链表中。这个内核中的双向链表与 ngx_queue_t 容器几乎
是完全相同的（Nginx 代码与 Linux 内核代码很相似），我们可以参照着理解。在 epoll 中，对
于每一个事件都会建立一个 epitem 结构体，如下所示。

struct epitem {
…
// 红黑树节点，与第
7 章中的
ngx_rbtree_node_t 红黑树节点相似
struct rb_node rbn;
// 双向链表节点，与第
7 章中的
ngx_queue_t 双向链表节点相似
struct list_head rdllink;
// 事件句柄等信息
struct epoll_filefd ffd;
// 指向其所属的
eventpoll 对象
struct eventpoll \*ep;
// 期待的事件类型
struct epoll_event event;
…
};
这里包含每一个事件对应着的信息。

当调用 epoll_wait 检查是否有发生事件的连接时，只是检查 eventpoll 对象中的 rdllist 双向
链表是否有 epitem 元素而已，如果 rdllist 链表不为空，则把这里的事件复制到用户态内存中，
同时将事件数量返回给用户。因此，epoll_wait 的效率非常高。epoll_ctl 在向 epoll 对象中添
加、修改、删除事件时，从 rbr 红黑树中查找事件也非常快，也就是说，epoll 是非常高效
的，它可以轻易地处理百万级别的并发连接。

9.6.2 如何使用 epoll
epoll 通过下面 3 个 epoll 系统调用为用户提供服务。

（1）epoll_create 系统调用
epoll_create 在 C 库中的原型如下。

int epoll_create(int size);
epoll_create 返回一个句柄，之后 epoll 的使用都将依靠这个句柄来标识。参数 size 是告诉
epoll 所要处理的大致事件数目。不再使用 epoll 时，必须调用 close 关闭这个句柄。

注意 size 参数只是告诉内核这个 epoll 对象会处理的事件大致数目，而不是能够处理
的事件的最大个数。在 Linux 最新的一些内核版本的实现中，这个 size 参数没有任何意义。

（2）epoll_ctl 系统调用
epoll_ctl 在 C 库中的原型如下。

int epoll_ctl(int epfd,int op,int fd,struct epoll_event\* event);
epoll_ctl 向 epoll 对象中添加、修改或者删除感兴趣的事件，返回 0 表示成功，否则返回–
1，此时需要根据 errno 错误码判断错误类型。epoll_wait 方法返回的事件必然是通过 epoll_ctl
添加到 epoll 中的。参数 epfd 是 epoll_create 返回的句柄，而 op 参数的意义见表 9-2。

表 9-2 epoll_ctl 系统调用中第 2 个参数的取值意义
第 3 个参数 fd 是待监测的连接套接字，第 4 个参数是在告诉 epoll 对什么样的事件感兴趣，
它使用了 epoll_event 结构体，在上文介绍过的 epoll 实现机制中会为每一个事件创建 epitem 结
构体，而在 epitem 中有一个 epoll_event 类型的 event 成员。下面看一下 epoll_event 的定义。

struct epoll_event{
\_\_uint32_t events;
epoll_data_t data;
};
events 的取值见表 9-3。

表 9-3 epoll_event 中 events 的取值意义
而 data 成员是一个 epoll_data 联合，其定义如下。

typedef union epoll_data {
void \*ptr;
int fd;
uint32_t u32;
uint64_t u64;
} epoll_data_t;
可见，这个 data 成员还与具体的使用方式相关。例如，ngx_epoll_module 模块只使用了联
合中的 ptr 成员，作为指向 ngx_connection_t 连接的指针。

（3）epoll_wait 系统调用
epoll_wait 在 C 库中的原型如下。

int epoll_wait(int epfd,struct epoll_event\* events,int maxevents,int timeout);
收集在 epoll 监控的事件中已经发生的事件，如果 epoll 中没有任何一个事件发生，则最多
等待 timeout 毫秒后返回。epoll_wait 的返回值表示当前发生的事件个数，如果返回 0，则表示
本次调用中没有事件发生，如果返回–1，则表示出现错误，需要检查 errno 错误码判断错误类
型。第 1 个参数 epfd 是 epoll 的描述符。第 2 个参数 events 则是分配好的 epoll_event 结构体数组，
epoll 将会把发生的事件复制到 events 数组中（events 不可以是空指针，内核只负责把数据复制
到这个 events 数组中，不会去帮助我们在用户态中分配内存。内核这种做法效率很高）。第 3
个参数 maxevents 表示本次可以返回的最大事件数目，通常 maxevents 参数与预分配的 events 数
组的大小是相等的。第 4 个参数 timeout 表示在没有检测到事件发生时最多等待的时间（单位
为毫秒），如果 timeout 为 0，则表示 epoll_wait 在 rdllist 链表中为空，立刻返回，不会等待。

epoll 有两种工作模式：LT（水平触发）模式和 ET（边缘触发）模式。默认情况下，
epoll 采用 LT 模式工作，这时可以处理阻塞和非阻塞套接字，而表 9-3 中的 EPOLLET 表示可以
将一个事件改为 ET 模式。ET 模式的效率要比 LT 模式高，它只支持非阻塞套接字。ET 模式与
LT 模式的区别在于，当一个新的事件到来时，ET 模式下当然可以从 epoll_wait 调用中获取到
这个事件，可是如果这次没有把这个事件对应的套接字缓冲区处理完，在这个套接字没有新
的事件再次到来时，在 ET 模式下是无法再次从 epoll_wait 调用中获取这个事件的；而 LT 模式
则相反，只要一个事件对应的套接字缓冲区还有数据，就总能从 epoll_wait 中获取这个事
件。因此，在 LT 模式下开发基于 epoll 的应用要简单一些，不太容易出错，而在 ET 模式下事
件发生时，如果没有彻底地将缓冲区数据处理完，则会导致缓冲区中的用户请求得不到响
应。默认情况下，Nginx 是通过 ET 模式使用 epoll 的，在下文中就可以看到相关内容。

9.6.3 ngx_epoll_module 模块的实现
本节主要介绍事件驱动模块接口与 epoll 用法是如何结合起来发挥作用的。首先看一下
ngx_epoll_module 模块究竟对哪些配置项感兴趣，其中 ngx_epoll_commands 数组指明了影响其
可定制性的两个配置项。

static ngx*command_t ngx_epoll_commands[] = {
/*在调用
epoll*wait 时，将由第
2 和第
3 个参数告诉
Linux 内核一次最多可返回多少个事件。这个配置项表示调用一次
epoll_wait 时最多可以返回的事件数，当然，它也会预分配那么多
epoll_event 结构体用于存储事件
*/
{ ngx*string("epoll_events"),
NGX_EVENT_CONF|NGX_CONF_TAKE1,
ngx_conf_set_num_slot,
0,
offsetof(ngx_epoll_conf_t, events),
NULL },
/*指明在开启异步
I/O 且使用
io*setup 系统调用初始化异步
I/O 上下文环境时
,初始分配的异步
I/O 事件个数，详见
9.9 节
*/
{ ngx_string("worker_aio_requests"),
NGX_EVENT_CONF|NGX_CONF_TAKE1,
ngx_conf_set_num_slot,
0,
offsetof(ngx_epoll_conf_t, aio_requests), NULL },
ngx_null_command
};
上面使用了预分配的 ngx_conf_set_num_slot 方法来解析这两个配置项，下面看一下存储
配置项的结构体 ngx_epoll_conf_t。

ngx_epoll_conf_t。

typedef struct {
ngx_uint_t events;
ngx_uint_t aio_requests;
} ngx_epoll_conf_t;
其中，events 是调用 epoll_wait 方法时传入的第 3 个参数 maxevents，而第 2 个参数 events 数
组的大小也是由它决定的，下面将在 ngx_epoll_init 方法中初始化这个数组。

接下来看一下 epoll 是如何定义 ngx_event_module_t 事件模块接口的，代码如下。

static ngx_str_t epoll_name = ngx_string("epoll"); ngx_event_module_t ngx_epoll_module_ctx = {
&epoll_name,
ngx_epoll_create_conf,
ngx_epoll_init_conf,
{
// 对应于
ngx_event_actions_t 中的
add 方法
ngx_epoll_add_event,
// 对应于
ngx_event_actions_t 中的
del 方法
ngx_epoll_del_event,
// 对应于
ngx_event_actions_t 中的
enable 方法，与
add 方法一致
ngx_epoll_add_event,
// 对应于
ngx_event_actions_t 中的
disable 方法，与
del 方法一致
ngx_epoll_del_event,
// 对应于
ngx_event_actions_t 中的
add_conn 方法
ngx_epoll_add_connection,
// 对应于
ngx_event_actions_t 中的
del_conn 方法
ngx_epoll_del_connection,
// 未实现
ngx_event_actions_t 中的
process_changes 方法
NULL,
// 对应于
ngx_event_actions_t 中的
process_events 方法
ngx_epoll_process_events,
// 对应于
ngx_event_actions_t 中的
init 方法
ngx_epoll_init,
// 对应于
ngx_event_actions_t 中的
done 方法
ngx_epoll_done,
}
};
其中，ngx_epoll_create_conf 方法和 ngx_epoll_init_conf 方法只是为了解析配置项，略过不
提，下面重点看一下 ngx_event_actions_t 中的 10 个接口是如何实现的。

首先从实现 init 接口的 ngx_epoll_init 方法讲起。ngx_epoll-init 方法是在什么时候被调用的
呢？在图 9-4 的第 4 步中它会被调用，也就是 Nginx 的启动过程中。ngx_epoll_init 方法主要做了
两件事：
1）调用 epoll_create 方法创建 epoll 对象。

2）创建 event_list 数组，用于进行 epoll_wait 调用时传递内核态的事件。

event_list 数组就是用于在 epoll_wait 调用中接收事件的参数，如下所示。

static int ep = -1;
static struct epoll_event \*event_list;
static ngx_uint_t nevents;
其中，ep 是 epoll 对象的描述符，nevents 是上面说到的 epoll_events 配置项参数，它既指明
了 epoll_wait 一次返回的最大事件数，也告诉了 event_list 应该分配的数组大小。ngx_epoll_init
方法代码如下所示。

static ngx*int_t ngx_epoll_init(ngx_cycle_t *cycle, ngx_msec_t timer) {
ngx_epoll_conf_t *epcf;
/*获取
create*conf 中生成的
ngx_epoll_conf_t 结构体，它已经被赋予解析完配置文件后的值。详细内容可参见
9.4.1 节中关于
ngx_event_get_conf 宏的用法
*/
epcf = ngx*event_get_conf(cycle-\>conf_ctx, ngx_epoll_module); if (ep == -1) {
/*调用
epoll*create 在内核中创建
epoll 对象。上文已经讲过，参数
size 不是用于指明
epoll 能够处理的最大事件个数，因为在许多
Linux 内核版本中，
epoll 是不处理这个参数的，所以设为
cycle-\> connection_n/2（而不是
cycle-\>connection_n）也不要紧
*/
ep = epoll_create(cycle-\>connection_n/2); if (ep == -1) {
ngx_log_error(NGX_LOG_EMERG, cycle-\>log, ngx_errno, "epoll_create() failed"); return NGX_ERROR;
}
if (NGX_HAVE_FILE_AIO)
// 异步
I/O 内容可参见
9.9 节
ngx_epoll_aio_init(cycle, epcf);
endif
}
if (nevents \< epcf-\>events) {
if (event_list) {
ngx_free(event_list);
}
// 初始化
event_list 数组。数组的个数是配置项
epoll_events 的参数
event_list = ngx_alloc(sizeof(struct epoll_event) * epcf-\>events, cycle-\>log); if (event_list == NULL) {
return NGX_ERROR;
}
}
// nevents 也是配置项
epoll_events 的参数
nevents = epcf-\>events;
// 指明读写
I/O 的方法，本章不做具体说明
ngx_io = ngx_os_io;
// 设置
ngx_event_actions 接口
ngx_event_actions = ngx_epoll_module_ctx.actions; #if (NGX_HAVE_CLEAR_EVENT)
/*默认是采用
ET 模式来使用
epoll 的，
NGX_USE_CLEAR_EVENT 宏实际上就是在告诉
Nginx 使用
ET 模式
\*/
ngx_event_flags = NGX_USE_CLEAR_EVENT
else
ngx_event_flags = NGX_USE_LEVEL_EVENT
endif
|NGX_USE_GREEDY_EVENT
|NGX_USE_EPOLL_EVENT; return NGX_OK;
}
ngx_event_actions 是在 Nginx 事件框架处理事件时封装的接口，我们会在 9.8 节中说明它的
用法。

对于 epoll 而言，并没有 enable 事件和 disable 事件的概念，另外，从 ngx_epoll_module_ctx
结构体中可以看出，enable 和 add 接口都是使用 ngx_epoll_add_event 方法实现的，而 disable 和
del 接口都是使用 ngx_epoll_del_event 方法实现的。下面以 ngx_epoll_add_event 方法为例介绍一
下它们是如何调用 epoll_ctl 向 epoll 中添加事件或从 epoll 中删除事件的。

static ngx*int_t
ngx_epoll_add_event(ngx_event_t *ev, ngx*int_t event, ngx_uint_t flags) {
int op;
uint32_t events, prev;
ngx_event_t *e;
ngx*connection_t *c;
struct epoll*event ee;
// 每个事件的
data 成员都存放着其对应的
ngx_connection_t 连接
c = ev-\>data;
/*下面会根据
event 参数确定当前事件是读事件还是写事件，这会决定
events 是加上
EPOLLIN 标志位还是
EPOLLOUT 标志位
\_/
events = (uint32*t) event;
…
// 根据
active 标志位确定是否为活跃事件，以决定到底是修改还是添加事件
if (e-\>active) {
op = EPOLL_CTL_MOD;
…
} else {
op = EPOLL_CTL_ADD;
}
// 加入
flags 参数到
events 标志位中
ee.events = events | (uint32_t) flags; /\_ptr 成员存储的是
ngx_connection_t 连接，可参见
9.6.2 节中
epoll 的使用方式。在
9.2 节中曾经提到过事件的
instance 标志位，下面就配合
ngx_epoll_process_events 方法说明它的用法
*/
ee.data.ptr = (void \_) ((uintptr_t) c | ev-\>instance); // 调用
epoll_ctl 方法向
epoll 中添加事件或者在
epoll 中修改事件
if (epoll_ctl(ep, op, c-\>fd, &ee) == -1) {
ngx_log_error(NGX_LOG_ALERT, ev-\>log, ngx_errno, "epoll_ctl(%d, %d) failed", op, c-\>fd); return NGX_ERROR;
}
// 将事件的
active 标志位置为
1，表示当前事件是活跃的
ev-\>active = 1;
return NGX_OK;
}
ngx_epoll_del_event 方法也通过 epoll_ctl 删除 epoll 中的事件，具体代码这里不再罗列，读
者可参照 ngx_epoll_add_event 的实现理解其意义。

对于 ngx_epoll_add_connection 方法和 ngx_epoll_del_connection 方法，也是调用 epoll_ctl 方
法向 epoll 中添加事件或者在 epoll 中删除事件的，只是每一个连接都对应读/写事件。因此，
ngx_epoll_add_connection 方法和 ngx_epoll_del_connection 方法在每次执行时也都是同时将每个
连接对应的读、写事件 active 标志位置为 1 的，这里将不再给出其代码。

对于事件的 instance 标志位，已经在 9.2 节中简单地介绍了它的意义，下面将结合
ngx_epoll_process_events 方法具体说明其意义。ngx_epoll_process_events 是实现了收集、分发
事件的 process_events 接口的方法，其主要代码如下所示。

static ngx*int_t ngx_epoll_process_events(ngx_cycle_t *cycle, ngx_msec_t timer, ngx_uint_t flags) {
int events;
uint32_t revents;
ngx_int_t instance, i;
ngx_event_t *rev, wev, *queue; ngx_connection_t *c;
/*调用
epoll*wait 获取事件。注意，
timer 参数是在
process_events 调用时传入的，在
9.7 和
9.8 节中会提到这个参数
*/
events = epoll*wait(ep, event_list, (int) nevents, timer); …
/*在
9.7 节中会介绍
Nginx 对时间的缓存和管理。当
flags 标志位指示要更新时间时，就是在这里更新的
*/
if (flags & NGX_UPDATE_TIME || ngx_event_timer_alarm) {
// 更新时间，参见
9.7.1 节
ngx_time_update();
}
…
// 遍历本次
epoll_wait 返回的所有事件
for (i = 0; i \< events; i++) {
/*对照着上面提到的
ngx*epoll_add_event 方法，可以看到
ptr 成员就是
ngx_connection_t 连接的地址，但最后
1 位有特殊含义，需要把它屏蔽掉
*/
c = event*list[i].data.ptr;
// 将地址的最后一位取出来，用
instance 变量标识
instance = (uintptr_t) c & 1;
/*无论是
32 位还是
64 位机器，其地址的最后
1 位肯定是
0，可以用下面这行语句把
ngx*connection_t 的地址还原到真正的地址值
*/
c = (ngx_connection_t *) ((uintptr_t) c & (uintptr_t) ~1); // 取出读事件
rev = c-\>read;
// 判断这个读事件是否为过期事件
if (c-\>fd == -1 || rev-\>instance != instance) {
/*当
fd 套接字描述符为
-1 或者
instance 标志位不相等时，表示这个事件已经过期了，不用处理
*/
continue;
}
// 取出事件类型
revents = event_list[i].events;
…
// 如果是读事件且该事件是活跃的
if ((revents & EPOLLIN) && rev-\>active) {
…
// flags 参数中含有
NGX_POST_EVENTS 表示这批事件要延后处理
if (flags & NGX_POST_EVENTS) {
/*如果要在
post 队列中延后处理该事件，首先要判断它是新连接事件还是普通事件，以决定把它加入到
ngx_posted_accept_events 队列或者
ngx_posted_events 队列中。关于
post 队列中的事件何时执行，可参见
9.8 节内容
*/
queue = (ngx_event_t \*\*) (rev-\>accept &ngx_posted_accept_events : &ngx_posted_events); // 将这个事件添加到相应的延后执行队列中
ngx_locked_post_event(rev, queue); } else {
// 立即调用读事件的回调方法来处理这个事件
rev-\>handler(rev);
}
}
// 取出写事件
wev = c-\>write;
if ((revents & EPOLLOUT) && wev-\>active) {
// 判断这个读事件是否为过期事件
if (c-\>fd == -1 || wev-\>instance != instance) {
/*当
fd 套接字描述符为
-1 或者
instance 标志位不相等时，表示这个事件已经过期了，不用处理
\*/
continue;
}
…
if (flags & NGX_POST_EVENTS) {
// 将这个事件添加到
post 队列中延后处理
ngx_locked_post_event(wev, &ngx_posted_events); } else {
// 立即调用这个写事件的回调方法来处理这个事件
wev-\>handler(wev);
}
}
}
…
return NGX_OK;
}
ngx_epoll_process_events 方法会收集当前触发的所有事件，对于不需要加入到 post 队列延
后处理的事件，该方法会立刻执行它们的回调方法，这其实是在做分发事件的工作，只是它
会在自己的进程中调用这些回调方法而已，因此，每一个回调方法都不能导致进程休眠或者
消耗太多的时间，以免 epoll 不能即时地处理其他事件。

instance 标志位为什么可以判断事件是否过期？从上面的代码可以看出，instance 标志位
的使用其实很简单，它利用了指针的最后一位一定是 0 这一特性。既然最后一位始终都是 0，
那么不如用来表示 instance。这样，在使用 ngx_epoll_add_event 方法向 epoll 中添加事件时，就
把 epoll_event 中联合成员 data 的 ptr 成员指向 ngx_connection_t 连接的地址，同时把最后一位置
为这个事件的 instance 标志。而在 ngx_epoll_process_events 方法中取出指向连接的 ptr 地址时，
先把最后一位 instance 取出来，再把 ptr 还原成正常的地址赋给 ngx_connection_t 连接。这样，
instance 究竟放在何处的问题也就解决了。

那么，过期事件又是怎么回事呢？举个例子，假设 epoll_wait 一次返回 3 个事件，在第 1
个事件的处理过程中，由于业务的需要，所以关闭了一个连接，而这个连接恰好对应第 3 个
事件。这样的话，在处理到第 3 个事件时，这个事件就已经是过期事件了，一旦处理必然出
错。既然如此，把关闭的这个连接的 fd 套接字置为–1 能解决问题吗？答案是不能处理所有情
况。

下面先来看看这种貌似不可能发生的场景到底是怎么发生的：假设第 3 个事件对应的
ngx_connection_t 连接中的 fd 套接字原先是 50，处理第 1 个事件时把这个连接的套接字关闭了，
同时置为–1，并且调用 ngx_free_connection 将该连接归还给连接池。在
ngx_epoll_process_events 方法的循环中开始处理第 2 个事件，恰好第 2 个事件是建立新连接事
件，调用 ngx_get_connection 从连接池中取出的连接非常可能就是刚刚释放的第 3 个事件对应
的连接。由于套接字 50 刚刚被释放，Linux 内核非常有可能把刚刚释放的套接字 50 又分配给
新建立的连接。因此，在循环中处理第 3 个事件时，这个事件就是过期的了！它对应的事件
是关闭的连接，而不是新建立的连接。

如何解决这个问题？依靠 instance 标志位。当调用 ngx_get_connection 从连接池中获取一个
新连接时，instance 标志位就会置反，代码如下所示。

ngx_connection_t *
ngx_get_connection(ngx_socket_t s, ngx_log_t *log) {
…
// 从连接池中获取一个连接
ngx_connection_t \*c;
c = ngx_cycle-\>free_connections;
…
rev = c-\>read;
wev = c-\>write;
…
instance = rev-\>instance;
// 将
instance 标志位置为原来的相反值
rev-\>instance = !instance;
wev-\>instance = !instance;
…
return c;
}
这样，当这个 ngx_connection_t 连接重复使用时，它的 instance 标志位一定是不同的。因
此，在 ngx_epoll_process_events 方法中一旦判断 instance 发生了变化，就认为这是过期事件而
不予处理。这种设计方法是非常值得读者学习的，因为它几乎没有增加任何成本就很好地解
决了服务器开发时一定会出现的过期事件问题。

目前，在 ngx_event_actions_t 接口中，所有事件模块都没有实现 process_changes 方法。

done 接口是由 ngx_epoll_done 方法实现的，在 Nginx 退出服务时它会得到调用。ngx_epoll_done
主要是关闭 epoll 描述符 ep，同时释放 event_list 数组。

了解了 ngx_epoll_module_ctx 中所有接口的实现后，ngx_epoll_module 模块的定义就非常
简单了，如下所示。

ngx*module_t ngx_epoll_module = {
NGX_MODULE_V1,
&ngx_epoll_module_ctx, /* module context _/
ngx_epoll_commands, /_ module directives _/
NGX_EVENT_MODULE, /_ module type _/
NULL, /_ init master _/
NULL, /_ init module _/
NULL, /_ init process _/
NULL, /_ init thread _/
NULL, /_ exit thread _/
NULL, /_ exit process _/
NULL, /_ exit master \_/
NGX_MODULE_V1_PADDING
};
这里不需要再实现 ngx_module_t 接口中的 7 个回调方法了。

至此，我们完整地介绍了 ngx_epoll_module 模块是如何实现事件驱动机制的内容的。事
实上，其他事件驱动模块的实现与 ngx_epoll_module 模块的差别并不是很大，读者可以参照
本节内容阅读其他事件模块的源代码。

9.7 定时器事件
Nginx 实现了自己的定时器触发机制，它与 epoll 等事件驱动模块处理的网络事件不同：
在网络事件中，网络事件的触发是由内核完成的，内核如果支持 epoll 就可以使用
ngx_epoll_module 模块驱动事件，内核如果仅支持 select 那就得使用 ngx_select_module 模块驱动
事件；定时器事件则完全是由 Nginx 自身实现的，它与内核完全无关。那么，所有事件的定
时器是如何组织起来的呢？在事件超时后，定时器是如何触发事件的呢？读者将在 9.7.2 节中
看到定时器事件的设计，但首先需要弄清楚 Nginx 的时间是如何管理的。Nginx 与一般的服务
器不同，出于性能的考虑（不需要每次获取时间都调用 gettimeofday 方法），Nginx 使用的时
间是缓存在其内存中的，这样，在 Nginx 模块获取时间时，只是获取内存中的几个整型变量
而已。这个缓存的时间是如何更新的呢？又是在什么时刻更新的呢？这些问题读者会在 9.7.1
节中获得答案。

9.7.1 缓存时间的管理
Nginx 中的每个进程都会单独地管理当前时间，下面来看一下缓存的全局时间变量是什
么。ngx_time_t 结构体是缓存时间变量的类型，如下所示。

typedef struct {
// 格林威治时间
1970 年
1 月
1 日凌晨
0 点
0 分
0 秒到当前时间的秒数
time_t sec;
// sec 成员只能精确到秒，
msec 则是当前时间相对于
sec 的毫秒偏移量
ngx_uint_t msec;
// 时区
ngx_int_t gmtoff;
} ngx_time_t;
可以看到，ngx_time_t 是精确到毫秒的。当然，ngx_time_t 结构用起来并不是那么方便，
作为 Web 服务器，很多时候要用到可读性较强的规范的时间字符串，因此，Nginx 定义了以下
全局变量用于缓存时间，代码如下。

// 格林威治时间
1970 年
1 月
1 日凌晨
0 点
0 分
0 秒到当前时间的毫秒数
volatile ngx_msec_t ngx_current_msec; // ngx_time_t 结构体形式的当前时间
volatile ngx_time_t *ngx_cached_time; /*用于记录
error_log 的当前时间字符串，它的格式类似于：
"1970/09/28 12:00:00"*/
volatile ngx_str_t ngx_cached_err_log_time; /*用于
HTTP 相关的当前时间字符串，它的格式类似于：
"Mon, 28 Sep 1970 06:00:00 GMT"*/
volatile ngx_str_t ngx_cached_http_time; /*用于记录
HTTP 日志的当前时间字符串，它的格式类似于：
"28/Sep/1970:12:00:00 +0600"\*/
volatile ngx_str_t ngx_cached_http_log_time; // 以
ISO 8601 标准格式记录下的字符串形式的当前时间
volatile ngx_str_t ngx_cached_http_log_iso8601;
Nginx 为用户提供了 6 种当前时间的表示形式，这已经足够用了。Nginx 缓存时间的操作
方法见表 9-4 所示。

表 9-4 Nginx 缓存时间的操作方法
ngx_tm_t 是标准的 tm 类型时间，下面先看一下 tm 时间是什么样的，代码如下。

struct tm {
// 秒

-   取值区间为
    [0,59]
    int tm_sec;
    // 分
-   取值区间为
    [0,59]
    int tm_min;
    // 时
-   取值区间为
    [0,23]
    int tm_hour;
    // 一个月中的日期
-   取值区间为
    [1,31]
    int tm_mday;
    // 月份（从一月开始，
    0 代表一月）
-   取值区间为
    [0,11]
    int tm_mon;
    // 年份，其值等于实际年份减去
    1900
    int tm_year;
    // 星期
-   取值区间为
    [0,6]，其中
    0 代表星期天，
    1 代表星期一，依此类推
    int tm_wday;
    /\*从每年的
    1 月
    1 日开始的天数
-   取值区间为
    [0,365]，其中
    0 代表
    1 月
    1 日，
    1 代表
    1 月
    2 日，依此类推
    */
    int tm_yday;
    /*夏令时标识符。在实行夏令时的时候，
    tm_isdst 为正；不实行夏令时的时候，
    tm_isdst 为
    0；在不了解情况时，
    tm_isdst 为负
    \*/
    int tm_isdst;
    };
    ngx_tm_t 与 tm 用法是完全一致的，如下所示。

typedef struct tm ngx_tm_t; #define ngx_tm_sec tm_sec #define ngx_tm_min tm_min #define ngx_tm_hour tm_hour #define ngx_tm_mday tm_mday #define ngx_tm_mon tm_mon #define ngx_tm_year tm_year #define ngx_tm_wday tm_wday #define ngx_tm_isdst tm_isdst
可以看到，ngx_tm_t 中类似 ngx_tm_sec 这样的成员与 tm_sec 是完全一致的。

这个缓存时间什么时候会更新呢？对于 worker 进程而言，除了 Nginx 启动时更新一次时
间外，任何更新时间的操作都只能由 ngx_epoll_process_events 方法（参见 9.6.3 节）执行。回
顾一下 ngx_epoll_process_events 方法的代码，当 flags 参数中有 NGX_UPDATE_TIME 标志位，
或者 ngx_event_timer_alarm 标志位为 1 时，就会调用 ngx_time_update 方法更新缓存时间。

9.7.2 缓存时间的精度
上文简单地介绍过缓存时间的更新策略，它是与 ngx_epoll_process_events 方法的调用频
率及其 flag 参数相关的。实际上，Nginx 还提供了设置更新缓存时间频率的功能（也就是至少
每隔 timer_resolution 毫秒必须更新一次缓存时间），通过在 nginx.conf 文件中的 timer_resolution
配置项可以设置更新的最小频率，这样就保证了缓存时间的精度。

下面看一下 timer_resolution 是如何起作用的。在图 9-4 的第 5 步中，ngx_event_core_module
模块初始化时会使用 setitimer 系统调用告诉内核每隔 timer_resolution 毫秒调用一次
ngx_timer_signal_handler 方法。而 ngx_timer_signal_handler 方法则会将 ngx_event_timer_alarm 标
志位设为 1，这样一来，一旦调用 ngx_epoll_process_events 方法，如果间隔的时间超过
timer_resolution 毫秒，肯定会更新缓存时间。

但如果很久都不调用 ngx_epoll_process_events 方法呢？例如，远超过 timer_resolution 毫秒
的时间内 ngx_epoll_process_events 方法都得不到调用，那时间精度如何保证呢？在这种情况
下，Nginx 只能从事件模块对 ngx_event_actions 中 process_events 接口的实现来保证时间精度
了。process_events 方法的第 2 个参数 timer 表示收集事件时的最长等待时间。例如，在 epoll 模
块下，这个 timer 就是 epoll_wait 调用时传入的超时时间参数。如果在设置了 timer_resolution
后，这个 timer 参数就是–1，它表示如果 epoll_wait 等调用检测不到已经发生的事件，将不等
待而是立刻返回，这样就控制了时间精度。当然，如果某个事件消费模块的回调方法执行时
占用的时间过长，时间精度还是难以得到保证的。

9.7.3 定时器的实现
定时器是通过一棵红黑树实现的。ngx_event_timer_rbtree 就是所有定时器事件组成的红
黑树，而 ngx_event_timer_sentinel 就是这棵红黑树的哨兵节点，如下所示。

ngx_thread_volatile ngx_rbtree_t ngx_event_timer_rbtree; static ngx_rbtree_node_t ngx_event_timer_sentinel;
这棵红黑树中的每个节点都是 ngx_event_t 事件中的 timer 成员，而 ngx_rbtree_node_t 节点的
关键字就是事件的超时时间，以这个超时时间的大小组成了二叉排序树
ngx_event_timer_rbtree。这样，如果需要找出最有可能超时的事件，那么将
ngx_event_timer_rbtree 树中最左边的节点取出来即可。只要用当前时间去比较这个最左边节
点的超时时间，就会知道这个事件有没有触发超时，如果还没有触发超时，那么会知道最少
还要经过多少毫秒满足超时条件而触发超时。先看一下定时器的操作方法，见表 9-5。

表 9-5 定时器的操作方法
事实上，还有两个宏与 ngx_event_add_timer 方法和 ngx_event_del_timer 方法的用法是完全
一样的，如下所示。

define ngx_add_timer ngx_event_add_timer #define ngx_del_timer ngx_event_del_timer
从表 9-5 可以看出，只要调用 ngx_event_expire_timers 方法就可以触发所有超时的事件，
在这个方法中，循环调用所有满足超时条件的事件的 handler 回调方法。那么，多久调用一次
ngx_event_expire_timers 方法呢？这个时间频率可以部分参照 ngx_event_find_timer 方法，因为
ngx_event_find_timer 会告诉用户下一个最近的超时事件多久后会发生。

在 9.8.5 节中，读者会看到 ngx_event_expire_timers 究竟什么时候会被调用。

9.8 事件驱动框架的处理流程
本节开始讨论事件处理流程。在 9.5.1 节中已经看到，图 9-4 的第 12 步会将监听连接的读
事件设为 ngx_event_accept 方法，在第 13 步会把监听连接的读事件添加到 ngx_epoll_module 事
件驱动模块中。这样，在执行 ngx_epoll_process_events 方法时，如果有新连接事件出现，则
会调用 ngx_event_accept 方法来建立新连接。在 9.8.1 节中将会讨论 ngx_event_accept 方法的执行
流程。

当然，建立连接其实没有那么简单。Nginx 出于充分发挥多核 CPU 架构性能的考虑，使
用了多个 worker 子进程监听相同端口的设计，这样多个子进程在 accept 建立新连接时会有争
抢，这会带来著名的“惊群”问题，子进程数量越多问题越明显，这会造成系统性能下降。在
9.8.2 节中，我们会讲到在建立新连接时 Nginx 是如何避免出现“惊群”现象的。

另外，建立连接时还会涉及负载均衡问题。在多个子进程争抢处理一个新连接事件时，
一定只有一个 worker 子进程最终会成功建立连接，随后，它会一直处理这个连接直到连接关
闭。那么，如果有的子进程很“勤奋”，它们抢着建立并处理了大部分连接，而有的子进程
则“运气不好”，只处理了少量连接，这对多核 CPU 架构下的应用是很不利的，因为子进程间
应该是平等的，每个子进程应该尽量地独占一个 CPU 核心。子进程间负载不均衡，必然影响
整个服务的性能。在 9.8.3 节中，我们会看到 Nginx 是如何解决负载均衡问题的。

实际上，上述问题的解决离不开 Nginx 的 post 事件处理机制。这个 post 事件是什么意思
呢？它表示允许事件延后执行。Nginx 设计了两个 post 队列，一个是由被触发的监听连接的读
事件构成的 ngx_posted_accept_events 队列，另一个是由普通读/写事件构成的
ngx_posted_events 队列。这样的 post 事件可以让用户完成什么样的功能呢？

-   将 epoll_wait 产生的一批事件，分到这两个队列中，让存放着新连接事件的
    ngx_posted_accept_events 队列优先执行，存放普通事件的 ngx_posted_events 队列最后执行，这
    是解决“惊群”和负载均衡两个问题的关键。

-   如果在处理一个事件的过程中产生了另一个事件，而我们希望这个事件随后执行（不
    是立刻执行），就可以把它放到 post 队列中。在 9.8.3 节中会介绍 post 队列。

我们在 9.8.5 节中将本章的网络事件、定时器事件进行综合考虑，以说明
ngx_process_events_and_timers 事件框架执行流程是如何把连接的建立、事件的执行结合在一
起的。

9.8.1 如何建立新连接
上文提到过，处理新连接事件的回调函数是 ngx_event_accept，其原型如下。

void ngx_event_accept(ngx_event_t \*ev)
下面简单介绍一下它的流程，如图 9-6 所示。

下面对流程中的 7 个步骤进行说明。

1）首先调用 accept 方法试图建立新连接，如果没有准备好的新连接事件，
ngx_event_accept 方法会直接返回。

2）设置负载均衡阈值 ngx_accept_disabled，这个阈值是进程允许的总连接数的 1/8 减去空
闲连接数，它的具体用法参见 9.8.3 节。

图 9-6 ngx_event_accept 方法建立新连接的流程
3）调用 ngx_get_connection 方法由连接池中获取一个 ngx_connection_t 连接对象。

4）为 ngx_connection_t 中的 pool 指针建立内存池。在这个连接释放到空闲连接池时，释放
pool 内存池。

5）设置套接字的属性，如设为非阻塞套接字。

6）将这个新连接对应的读事件添加到 epoll 等事件驱动模块中，这样，在这个连接上如
果接收到用户请求 epoll_wait，就会收集到这个事件。

7）调用监听对象 ngx_listening_t 中的 handler 回调方法。ngx_listening_t 结构体的 handler 回
调方法就是当新的 TCP 连接刚刚建立完成时在这里调用的。

最后，如果监听事件的 available 标志位为 1，再次循环到第 1 步，否则 ngx_event_accept 方
法结束。事件的 available 标志位对应着 multi_accept 配置项。当 available 为 1 时，告诉 Nginx 一
次性尽量多地建立新连接，它的实现原理也就在这里。

9.8.2 如何解决“惊群”问题
只有打开了 accept_mutex 锁，才可以解决“惊群”问题。何谓“惊群”？就像上面说过的那
样，master 进程开始监听 Web 端口，fork 出多个 worker 子进程，这些子进程开始同时监听同一
个 Web 端口。一般情况下，有多少 CPU 核心，就会配置多少个 worker 子进程，这样所有的
worker 子进程都在承担着 Web 服务器的角色。在这种情况下，就可以利用每一个 CPU 核心可
以并发工作的特性，充分发挥多核机器的“威力”。但下面假定这样一个场景：没有用户连入
服务器，某一时刻恰好所有的 worker 子进程都休眠且等待新连接的系统调用（如
epoll_wait），这时有一个用户向服务器发起了连接，内核在收到 TCP 的 SYN 包时，会激活所
有的休眠 worker 子进程，当然，此时只有最先开始执行 accept 的子进程可以成功建立新连
接，而其他 worker 子进程都会 accept 失败。这些 accept 失败的子进程被内核唤醒是不必要的，
它们被唤醒后的执行很可能也是多余的，那么这一时刻它们占用了本不需要占用的系统资
源，引发了不必要的进程上下文切换，增加了系统开销。

也许很多操作系统的最新版本的内核已经在事件驱动机制中解决了“惊群”问题，但
Nginx 作为可移植性极高的 Web 服务器，还是在自身的应用层面上较好地解决了这一问题。既
然“惊群”是多个子进程在同一时刻监听同一个端口引起的，那么 Nginx 的解决方式也很简
单，它规定了同一时刻只能有唯一一个 worker 子进程监听 Web 端口，这样就不会发生“惊
群”了，此时新连接事件只能唤醒唯一正在监听端口的 worker 子进程。

可是如何限制在某一时刻仅能有一个子进程监听 Web 端口呢？下面看一下
ngx_trylock_accept_mutex 方法的实现。在打开 accept_mutex 锁的情况下，只有调用
ngx_trylock_accept_mutex 方法后，当前的 worker 进程才会去试着监听 web 端口，具体实现如下
所示。

ngx_int_t ngx_trylock_accept_mutex(ngx_cycle_t *cycle)
{
/*使用进程间的同步锁，试图获取
accept_mutex 锁。注意，
ngx_shmtx_trylock 返回
1 表示成功拿到锁，返回
0 表示获取锁失败。这个获取锁的过程是非阻塞的，此时一旦锁被其他
worker 子进程占用，
ngx_shmtx_trylock 方法会立刻返回（详见
14.8 节）
if (ngx_shmtx_trylock(&ngx_accept_mutex)) {
如果获取到
accept_mutex 锁，但
ngx_accept_mutex_held 为
1，则立刻返回。

ngx_accept_mutex_held 是一个标志位，当它为
1 时，表示当前进程已经获取到锁了
*/
if (ngx_accept_mutex_held
&& ngx_accept_events == 0
&& !(ngx_event_flags & NGX_USE_RTSIG_EVENT))
{
// ngx_accept_mutex 锁之前已经获取到了，立刻返回
return NGX_OK;
}
// 将所有监听连接的读事件添加到当前的
epoll 等事件驱动模块中
if (ngx_enable_accept_events(cycle) == NGX_ERROR) {
/*既然将监听句柄添加到事件驱动模块失败，就必须释放
ngx_accept_mutex 锁
ngx_shmtx_unlock(&ngx_accept_mutex);
return NGX_ERROR;
}
经过
ngx_enable_accept_events 方法的调用，当前进程的事件驱动模块已经开始监听所有的端口，这时需要把
ngx_accept_mutex_held 标志位置为
1，方便本进程的其他模块了解它目前已经获取到了锁
ngx_accept_events = 0;
ngx_accept_mutex_held = 1;
return NGX_OK;
}
如果
ngx_shmtx_trylock 返回
0，则表明获取
ngx_accept_mutex 锁失败，这时如果
ngx_accept_mutex_held 标志位还为
1，即当前进程还在获取到锁的状态，这当然是不正确的，需要处理
if (ngx_accept_mutex_held) {
ngx_disable_accept_events 会将所有监听连接的读事件从事件驱动模块中移除
if (ngx_disable_accept_events(cycle) == NGX_ERROR) {
return NGX_ERROR;
}
在没有获取到
ngx_accept_mutex 锁时，必须把
ngx_accept_mutex_held 置为
0\*/
ngx_accept_mutex_held = 0;
}
return NGX_OK;
}
在上面关于 ngx_trylock_accept_mutex 方法的源代码中，ngx_accept_mutex 实际上是 Nginx 进
程间的同步锁。第 14 章我们会详细介绍进程间的同步方式，目前只需要清楚
ngx_shmtx_trylock 方法是一个非阻塞的获取锁的方法即可。如果成功获取到锁，则返回 1，否
则返回 0。ngx_shmtx_unlock 方法负责释放锁。ngx_accept_mutex_held 是当前进程的一个全局变
量，如果为 1，则表示这个进程已经获取到了 ngx_accept_mutex 锁；如果为 0，则表示没有获
取到锁，这个标志位主要用于进程内各模块了解是否获取到了 ngx_accept_mutex 锁，具体定
义如下所示。

ngx_shmtx_t ngx_accept_mutex;
ngx_uint_t ngx_accept_mutex_held;
因此，在调用 ngx_trylock_accept_mutex 方法后，要么是唯一获取到 ngx_accept_mutex 锁且
其 epoll 等事件驱动模块开始监控 Web 端口上的新连接事件，要么是没有获取到锁，当前进程
不会收到新连接事件。

如果 ngx_trylock_accept_mutex 方法没有获取到锁，接下来调用事件驱动模块的
process_events 方法时只能处理已有的连接上的事件；如果获取到了锁，调用 process_events 方
法时就会既处理已有连接上的事件，也处理新连接的事件。这样的话，问题又来了，什么时
候释放 ngx_accept_mutex 锁呢？等到这批事件全部执行完吗？这当然是不可取的，因为这个
worker 进程上可能有许多活跃的连接，处理这些连接上的事件会占用很长时间，也就是说，
会有很长时间都没有释放 ngx_accept_mutex 锁，这样，其他 worker 进程就很难得到处理新连接
的机会。

如何解决长时间占用 ngx_accept_mutex 锁的问题呢？这就要依靠 ngx_posted_accept_events
队列和 ngx_posted_events 队列了。首先看下面这段代码：
if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) {
return;
}
if (ngx_accept_mutex_held) {
flags |= NGX_POST_EVENTS;
}
调用 ngx_trylock_accept_mutex 试图处理监听端口的新连接事件，如果
ngx_accept_mutex_held 为 1，就表示开始处理新连接事件了，这时将 flags 标志位加上
NGX_POST_EVENTS。这里的 flags 是在 9.6.3 节中列举的 ngx_epoll_process_events 方法中的第 3
个参数 flags。回顾一下这个方法中的代码，当 flags 标志位包含 NGX_POST_EVENTS 时是不会
立刻调用事件的 handler 回调方法的，代码如下所示。

if ((revents & EPOLLIN) && rev-\>active) {
if (flags & NGX_POST_EVENTS) {
queue = (ngx_event_t \*\*) (rev-\>accept &ngx_posted_accept_events : &ngx_posted_events);
ngx_locked_post_event(rev, queue);
} else {
rev-\>handler(rev);
}
}
对于写事件，也可以采用同样的处理方法。实际上，ngx_posted_accept_events 队列和
ngx_posted_events 队列把这批事件归类了，即新连接事件全部放到 ngx_posted_accept_events 队
列中，普通事件则放到 ngx_posted_events 队列中。这样，接下来会先处理
ngx_posted_accept_events 队列中的事件，处理完后就要立刻释放 ngx_accept_mutex 锁，接着再
处理 ngx_posted_events 队列中的事件（参见图 9-7），这样就大大减少了 ngx_accept_mutex 锁占
用的时间。

9.8.3 如何实现负载均衡
与“惊群”问题的解决方法一样，只有打开了 accept_mutex 锁，才能实现 worker 子进程间的
负载均衡。在图 9-6 的第 2 步中，初始化了一个全局变量 ngx_accept_disabled，它就是负载均衡
机制实现的关键阈值，实际上它就是一个整型数据。

ngx_int_t ngx_accept_disabled;
这个阈值是与连接池中连接的使用情况密切相关的，在图 9-6 的第 2 步中它会进行赋值，
如下所示。

ngx_accept_disabled = ngx_cycle-\>connection_n/8

-   ngx_cycle-\>free_connection_n;
    因此，在 Nginx 启动时，ngx_accept_disabled 的值就是一个负数，其值为连接总数的 7/8。

其实，ngx_accept_disabled 的用法很简单，当它为负数时，不会进行触发负载均衡操作；而
当 ngx_accept_disabled 是正数时，就会触发 Nginx 进行负载均衡操作了。Nginx 的做法也很简
单，就是当 ngx_accept_disabled 是正数时当前进程将不再处理新连接事件，取而代之的仅仅
是 ngx_accept_disabled 值减 1，如下所示。

if (ngx_accept_disabled \> 0) {
ngx_accept_disabled--;
} else {
if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) {
return;
}
…
｝
上面这段代码表明，在当前使用的连接到达总连接数的 7/8 时，就不会再处理新连接
了，同时，在每次调用 process_events 时都会将 ngx_accept_disabled 减 1，直到
ngx_accept_disabled 降到总连接数的 7/8 以下时，才会调用 ngx_trylock_accept_mutex 试图去处理
新连接事件。

因此，Nginx 各 worker 子进程间的负载均衡仅在某个 worker 进程处理的连接数达到它最大
处理总数的 7/8 时才会触发，这时该 worker 进程就会减少处理新连接的机会，这样其他较空闲
的 worker 进程就有机会去处理更多的新连接，以此达到整个 Web 服务的均衡处理效果。虽然
这样的机制不是很完美，但在维护一定程度上的负载均衡时，很好地避免了当某个 worker 进
程由于连接池耗尽而拒绝服务，同时，在其他 worker 进程上处理的连接还远未达到上限的问
题。因此，Nginx 将 accept_mutex 配置项默认设为 accept_mutex on。

9.8.4 post 事件队列
上文已经介绍过 post 事件的意义，本节来看一下 post 事件处理的实现方法。下面是两个
post 事件队列的定义：
ngx_thread_volatile ngx_event_t ngx_posted_accept_events;
ngx_thread_volatile ngx_event_t ngx_posted_events;
这两个指针都指向事件队列中的首个事件。这些事件间是以双向链表的形式组织成 post
事件队列的。注意，9.2 节中 ngx_event_t 结构体的 next 和 prev 成员仅用于 post 事件队列。

对于 post 事件队列的操作方法共有 4 个，见表 9-6。

在 9.6.3 节中已经介绍过 ngx_post_event 方法的应用，它会将事件添加到队列中，那么，
post 事件什么时候会执行呢？在 9.8.5 节我们就会介绍 ngx_event_process_posted 是如何被调用
的。

表 9-6 post 事件队列的操作方法
9.8.5 ngx_process_events_and_timers 流程
本节将综合上文相关内容，探讨 Nginx 事件框架处理的流程。

在图 8-7 中，每个 worker 进程都在 ngx_worker_process_cycle 方法中循环处理事件。图 8-7
中的处理分发事件实际上就是调用的 ngx_process_events_and_timers 方法，下面先看一下它的
定义：
void ngx_process_events_and_timers(ngx_cycle_t \*cycle);
循环调用 ngx_process_events_and_timers 方法就是在处理所有的事件，这正是事件驱动机
制的核心。顾名思义，ngx_process_events_and_timers 方法既会处理普通的网络事件，也会处
理定时器事件，在图 9-7 中，读者会看到在这个方法中到底做了哪些事情。

ngx_process_events_and_timers 方法中核心的操作主要有以下 3 个：

-   调用所使用的事件驱动模块实现的 process_events 方法，处理网络事件。

-   处理两个 post 事件队列中的事件，实际上就是分别调用
    ngx_event_process_posted(cycle,&ngx_posted_accept_events)和
    ngx_event_process_posted(cycle,&ngx_posted_events)方法（参见 9.8.4 节）。

-   处理定时器事件，实际上就是调用 ngx_event_expire_timers()方法（参见 9.7.3 节）。

后两项操作很清晰，而调用事件驱动模块的 process_events 方法时则需要设置两个关键参
数 timer 和 flags。Nginx 用一系列宏封装了 ngx_event_actions 接口中的方法，如下所示。

define ngx_process_changes ngx_event_actions.process_changes
define ngx_process_events ngx_event_actions.process_events
define ngx_done_events ngx_event_actions.done
define ngx_add_event ngx_event_actions.add
define ngx_del_event ngx_event_actions.del
define ngx_add_conn ngx_event_actions.add_conn
define ngx_del_conn ngx_event_actions.del_conn

图 9-7 ngx_process_events_and_timers 方法中的事件框架处理流程
在调用 ngx_process_events 时，传入的 timer 和 flags 会影响时间精度以及事件是否会在 post
队列中处理。下面简要分析一下图 9-7 中的 11 个步骤，其中前 6 个步骤都与参数 timer 和 flags 的
设置有关。

1）如果配置文件中使用了 timer_resolution 配置项，也就是 ngx_timer_resolution 值大于 0，
则说明用户希望服务器时间精确度为 ngx_timer_resolution 毫秒。这时，将 ngx_process_events
的 timer 参数设为–1，告诉 ngx_process_events 方法在检测事件时不要等待，直接搜集所有已经
就绪的事件然后返回；同时将 flags 参数初始化为 0，它是在告诉 ngx_process_events 没有任何
附加动作。

2）如果没有使用 timer_resolution，那么将调用 ngx_event_find_timer()方法（参见表 9-5）
获取最近一个将要触发的事件距离现在有多少毫秒，然后把这个值赋予 timer 参数，告诉
ngx_process_events 方法在检测事件时如果没有任何事件，最多等待 timer 毫秒就返回；将 flags
参数设置为 NGX_UPDATE_TIME，告诉 ngx_process_events 方法更新缓存的时间（参见 9.6.3 节
中 ngx_epoll_process_events 方法的源代码）。

3）如果在配置文件中使用 accept_mutex off 关闭 accept_mutex 锁，就直接跳到第 7 步，否则
检测负载均衡阈值变量 ngx_accept_disabled。如果 ngx_accept_disabled 是正数，则将其值减去
1，继续向下执行第 7 步。

4）如果 ngx_accept_disabled 是负数，表明还没有触发到负载均衡机制（参见 9.8.3 节），
此时要调用 ngx_trylock_accept_mutex 方法试图去获取 accept_mutex 锁（也就是 ngx_accept_mutex
变量表示的锁）。

5）如果获取到 accept_mutex 锁，也就是说，ngx_accept_mutex_held 标志位为 1，那么将
flags 参数加上 NGX_POST_EVENTS 标志，告诉 ngx_process_events 方法搜集到的事件没有直接
执行它的 handler 方法，而是分门别类地放到 ngx_posted_accept_events 队列和 ngx_posted_events
队列中。timer 参数保持不变。

6）如果没有获取到 accept_mutex 锁，则意味着既不能让当前 worker 进程频繁地试图抢
锁，也不能让它经过太长时间再去抢锁。这里有个简单的判断方法，如下所示。

if (timer == NGX_TIMER_INFINITE
|| timer \> ngx_accept_mutex_delay)
{
timer = ngx_accept_mutex_delay;
}
这意味着，即使开启了 timer_resolution 时间精度，也需要让 ngx_process_events 方法在没
有新事件的时候至少等待 ngx_accept_mutex_delay 毫秒再去试图抢锁。而没有开启时间精度
时，如果最近一个定时器事件的超时时间距离现在超过了 ngx_accept_mutex_delay 毫秒的话，
也要把 timer 设置为 ngx_accept_mutex_delay 毫秒，这是因为当前进程虽然没有抢到
accept_mutex 锁，但也不能让 ngx_process_events 方法在没有新事件的时候等待的时间超过
ngx_accept_mutex_delay 毫秒，这会影响整个负载均衡机制。

注意 ngx_accept_mutex_delay 变量与 nginx.conf 配置文件中的 accept_mutex_delay 配置项
的参数相关内容可参见 9.5 节。

7）调用 ngx_process_events 方法，并计算 ngx_process_events 执行时消耗的时间，如下所
示。

delta = ngx_current_msec;
(void) ngx_process_events(cycle, timer, flags);
delta = ngx_current_msec - delta;
其中，delta 是 ngx_process_events 执行时消耗的毫秒数，它会影响第 10 步中触发定时器的
执行。

8）如果 ngx_posted_accept_events 队列不为空，那么调用 ngx_event_process_posted 方法执
行 ngx_posted_accept_events 队列中需要建立新连接的事件。

9）如果 ngx_accept_mutex_held 标志位为 1，则表示当前进程获得了 accept_mutex 锁，而且
在第 8 步中也已经处理完了新连接事件，这时需要调用 ngx_shmtx_unlock 释放 accept_mutex 锁。

10）如果 ngx_process_events 执行时消耗的时间 delta 大于 0，而且这时可能有新的定时器
事件被触发，那么需要调用 ngx_event_expire_timers 方法处理所有满足条件的定时器事件。

11）如果 ngx_posted_events 队列不为空，则调用 ngx_event_process_posted 方法执行
ngx_posted_events 队列中的普通读/写事件。

至此，ngx_process_events_and_timers 方法执行完毕。注意，
ngx_process_events_and_timers 方法就是 Nginx 实际上处理 Web 服务的方法，所有业务的执行都
是由它开始的。ngx_process_events_and_timers 方法涉及 Nginx 完整的事件驱动机制，因此，它
也把之前介绍的内容整合在一起了，读者需要格外注意。

9.9 文件的异步 I/O
本章之前提到的事件驱动模块都是在处理网络事件，而没有涉及磁盘上文件的操作。本
节将讨论 Linux 内核 2.6.2x 之后版本中支持的文件异步 I/O，以及 ngx_epoll_module 模块是如何
与文件异步 I/O 配合提供服务的。这里提到的文件异步 I/O 并不是 glibc 库提供的文件异步 I/O。

glibc 库提供的异步 I/O 是基于多线程实现的，它不是真正意义上的异步 I/O。而本节说明的异
步 I/O 是由 Linux 内核实现，只有在内核中成功地完成了磁盘操作，内核才会通知进程，进而
使得磁盘文件的处理与网络事件的处理同样高效。

使用这种方式的前提是 Linux 内核版本中必须支持文件异步 I/O。当然，它带来的好处也
非常明显，Nginx 把读取文件的操作异步地提交给内核后，内核会通知 I/O 设备独立地执行操
作，这样，Nginx 进程可以继续充分地占用 CPU。而且，当大量读事件堆积到 I/O 设备的队列
中时，将会发挥出内核中“电梯算法”的优势，从而降低随机读取磁盘扇区的成本。

注意 Linux 内核级别的文件异步 I/O 是不支持缓存操作的，也就是说，即使需要操
作的文件块在 Linux 文件缓存中存在，也不会通过读取、更改缓存中的文件块来代替实际对
磁盘的操作，虽然从阻塞 worker 进程的角度上来说有了很大好转，但是对单个请求来说，还
是有可能降低实际处理的速度，因为原先可以从内存中快速获取的文件块在使用了异步 I/O
后则一定会从磁盘上读取。异步文件 I/O 是把“双刃剑”，关键要看使用场景，如果大部分
用户请求对文件的操作都会落到文件缓存中，那么不要使用异步 I/O，反之则可以试着使用
文件异步 I/O，看一下是否会为服务带来并发能力上的提升。

目前，Nginx 仅支持在读取文件时使用异步 I/O，因为正常写入文件时往往是写入内存中
就立刻返回，效率很高，而使用异步 I/O 写入时速度会明显下降。

9.9.1 Linux 内核提供的文件异步 I/O
Linux 内核提供了 5 个系统调用完成文件操作的异步 I/O 功能，见表 9-7。

表 9-7 Linux 内核提供的文件异步 I/O 操作方法
表 9-7 中列举的这 5 种方法提供了内核级别的文件异步 I/O 机制，使用前需要先调用
io_setup 方法初始化异步 I/O 上下文。虽然一个进程可以拥有多个异步 I/O 上下文，但通常有一
个就足够了。调用 io_setup 方法后会获得这个异步 I/O 上下文的描述符（aio_context_t 类型），
这个描述符和 epoll_create 返回的描述符一样，是贯穿始终的。注意，nr_events 只是指定了异
步 I/O 至少初始化的上下文容量，它并没有限制最大可以处理的异步 I/O 事件数目。为了便于
理解，不妨将 io_setup 与 epoll_create 进行对比，它们还是很相似的。

既然把 epoll 和异步 I/O 进行对比，那么哪些调用相当于 epoll_ctrl 呢？就是 io_submit 和
io_cancel。其中 io_submit 相当于向异步 I/O 中添加事件，而 io_cancel 则相当于从异步 I/O 中移
除事件。io_submit 中用到了一个结构体 iocb，下面简单地看一下它的定义。

struct iocb {
/_存储着业务需要的指针。例如，在
Nginx 中，这个字段通常存储着对应的
ngx_event_t 事件的指针。它实际上与
io_getevents 方法中返回的
io_event 结构体的
data 成员是完全一致的
_/
u*int64_t aio_data;
// 不需要设置
u_int32_t PADDED(aio_key, aio_reserved1); // 操作码，其取值范围是
io_iocb_cmd_t 中的枚举命令
u_int16_t aio_lio_opcode;
// 请求的优先级
int16_t aio_reqprio;
// 文件描述符
u_int32_t aio_fildes;
// 读
/写操作对应的用户态缓冲区
u_int64_t aio_buf;
// 读
/写操作的字节长度
u_int64_t aio_nbytes;
// 读
/写操作对应于文件中的偏移量
int64_t aio_offset;
// 保留字段
u_int64_t aio_reserved2;
/*表示可以设置为
IOCB*FLAG_RESFD，它会告诉内核当有异步
I/O 请求处理完成时使用
eventfd 进行通知，可与
epoll 配合使用，其在
Nginx 中的使用方法可参见
9.9.2 节
*/
u_int32_t aio_flags;
// 表示当使用
IOCB_FLAG_RESFD 标志位时，用于进行事件通知的句柄
u_int32_t aio_resfd;
};
因此，在设置好 iocb 结构体后，就可以向异步 I/O 提交事件了。aio_lio_opcode 操作码指
定了这个事件的操作类型，它的取值范围如下。

typedef enum io_iocb_cmd {
// 异步读操作
IO_CMD_PREAD = 0,
// 异步写操作
IO_CMD_PWRITE = 1,
// 强制同步
IO_CMD_FSYNC = 2,
// 目前未使用
IO_CMD_FDSYNC = 3,
// 目前未使用
IO_CMD_POLL = 5,
// 不做任何事情
IO_CMD_NOOP = 6,
} io_iocb_cmd_t;
在 Nginx 中，仅使用了 IO_CMD_PREAD 命令，这是因为目前仅支持文件的异步 I/O 读取，
不支持异步 I/O 的写入。这其中一个重要的原因是文件的异步 I/O 无法利用缓存，而写文件操
作通常是落到缓存中的，Linux 存在统一将缓存中“脏”数据刷新到磁盘的机制。

这样，使用 io_submit 向内核提交了文件异步 I/O 操作的事件后，再使用 io_cancel 则可以将
已经提交的事件取消。

如何获取已经完成的异步 I/O 事件呢？io_getevents 方法可以做到，它相当于 epoll 中的
epoll_wait 方法。这里用到了 io_event 结构体，下面看一下它的定义。

struct io_event {
// 与提交事件时对应的
iocb 结构体中的
aio_data 是一致的
uint64_t data;
// 指向提交事件时对应的
iocb 结构体
uint64_t obj;
// 异步
I/O 请求的结构。

res 大于或等于
0 时表示成功，小于
0 时表示失败
int64_t res;
// 保留字段
int64_t res2;
};
这样，根据获取的 io_event 结构体数组，就可以获得已经完成的异步 I/O 操作了，特别是
iocb 结构体中的 aio_data 成员和 io_event 中的 data，可用于传递指针，也就是说，业务中的数
据结构、事件完成后的回调方法都在这里。

进程退出时需要调用 io_destroy 方法销毁异步 I/O 上下文，这相当于调用 close 关闭 epoll 的
描述符。

Linux 内核提供的文件异步 I/O 机制用法非常简单，它充分利用了在内核中 CPU 与 I/O 设备
是各自独立工作的这一特性，在提交了异步 I/O 操作后，进程完全可以做其他工作，直到空
闲再来查看异步 I/O 操作是否完成。

9.9.2 ngx_epoll_module 模块中实现的针对文件的异步 I/O
在 Nginx 中，文件异步 I/O 事件完成后的通知是集成到 epoll 中的，它是通过 9.9.1 节中介绍
的 IOCB_FLAG_RESFD 标志位完成的。下面看看文件异步 I/O 事件在 ngx_epoll_module 模块中
是如何实现的，其中在文件异步 I/O 机制中定义的全局变量如下。

// 用于通知异步
I/O 事件的描述符，它与
iocb 结构体中的
aio*resfd 成员是一致的
int ngx_eventfd = -1;
// 异步
I/O 的上下文，全局唯一，必须经过
io_setup 初始化才能使用
aio_context_t ngx_aio_ctx = 0;
/*异步
I/O 事件完成后进行通知的描述符，也就是
ngx*eventfd 所对应的
ngx_event_t 事件
*/
static ngx*event_t ngx_eventfd_event; /*异步
I/O 事件完成后进行通知的描述符
ngx*eventfd 所对应的
ngx_connection_t 连接
*/
static ngx_connection_t ngx_eventfd_conn;
在 9.6.3 节的 ngx_epoll_init 代码中，在 epoll_create 执行完成后如果开启了文件异步 I/O 功
能，则会调用 ngx_epoll_aio_init 方法。现在详细描述一下 ngx_epoll_aio_init 方法中做了些什
么，如下所示。

define SYS*eventfd 323
static void ngx_epoll_aio_init(ngx_cycle_t *cycle, ngx_epoll_conf_t *epcf) {
int n;
struct epoll_event ee;
// 使用
Linux 中第
323 个系统调用获取一个描述符句柄
ngx_eventfd = syscall(SYS_eventfd, 0); …
// 设置
ngx_eventfd 为无阻塞
if (ioctl(ngx_eventfd, FIONBIO, &n) == -1) {
…
}
// 初始化文件异步
I/O 的上下文
if (io_setup(epcf-\>aio_requests, &ngx_aio_ctx) == -1) {
…
}
/*设置用于异步
I/O 完成通知的
ngx*eventfd_event 事件，它与
ngx_eventfd_conn 连接是对应的
*/
ngx_eventfd_event.data = &ngx_eventfd_conn; // 在异步
I/O 事件完成后，使用
ngx_epoll_eventfd_handler 方法处理
ngx_eventfd_event.handler = ngx_epoll_eventfd_handler; ngx_eventfd_event.log = cycle-\>log; ngx_eventfd_event.active = 1; //
ngx_eventfd_conn 连接
ngx_eventfd_conn.fd = ngx_eventfd; // ngx_eventfd_conn 连接的读事件就是上面的
ngx_eventfd_event
ngx_eventfd_conn.read = &ngx_eventfd_event; ngx_eventfd_conn.log = cycle-\>log; ee.events = EPOLLIN|EPOLLET;
ee.data.ptr = &ngx_eventfd_conn; // 向
epoll 中添加到异步
I/O 的通知描述符
ngx_eventfd
if (epoll_ctl(ep, EPOLL_CTL_ADD, ngx_eventfd, &ee) != -1) {
return;
}
…
}
这样，ngx_epoll_aio_init 方法会把异步 I/O 与 epoll 结合起来，当某一个异步 I/O 事件完成
后，ngx_eventfd 句柄就处于可用状态，这样 epoll_wait 在返回 ngx_eventfd_event 事件后就会调
用它的回调方法 ngx_epoll_eventfd_handler 处理已经完成的异步 I/O 事件，下面看一下
ngx_epoll_eventfd_handler 方法主要在做些什么，代码如下所示。

static void ngx*epoll_eventfd_handler(ngx_event_t *ev) {
int n, events;
uint64_t ready;
ngx_event_t *e;
// 一次性最多处理
64 个事件
struct io_event event[64];
struct timespec ts;
/*获取已经完成的事件数目，并设置到
ready 中，注意，这个
ready 是可以大于
64 的
\_/
n = read(ngx_eventfd, &ready, 8); …
// ready 表示还未处理的事件。当
ready 大于
0 时继续处理
while (ready) {
// 调用
io_getevents 获取已经完成的异步
I/O 事件
events = io_getevents(ngx_aio_ctx, 1, 64, event, &ts); if (events \> 0) {
// 将
ready 减去已经取出的事件
ready -= events;
// 处理
event 数组里的事件
for (i = 0; i \< events; i++) {
// data 成员指向这个异步
I/O 事件对应着的实际事件
e = (ngx_event_t \*) (uintptr_t) event[i].data; …
// 将该事件放到
ngx_posted_events 队列中延后执行
ngx_post_event(e, &ngx_posted_events); }
continue;
}
if (events == 0) {
return;
}
return;
}
}
整个网络事件的驱动机制就是这样通过 ngx_eventfd 通知描述符和
ngx_epoll_eventfd_handler 回调方法，并与文件异步 I/O 事件结合起来的。

那么，怎样向异步 I/O 上下文中提交异步 I/O 操作呢？看看 ngx_linux_aio_read.c 文件中的
ngx_file_aio_read 方法，在打开文件异步 I/O 后，这个方法将会负责磁盘文件的读取，如下所
示。

ssize*t ngx_file_aio_read(ngx_file_t file, u_char buf, size_t size, off_t offset, ngx_pool_t *pool) {
ngx_err_t err;
struct iocb *piocb[1];
ngx_event_t *ev;
ngx_event_aio_t *aio;
…
aio = file-\>aio;
ev = &aio-\>event;
…
ngx_memzero(&aio-\>aiocb, sizeof(struct iocb)); /*设置
9.9.1 节中介绍过的
iocb 结构体，这里的
aiocb 成员就是
iocb 类型。注意，
aio*data 已经设置为这个
ngx_event_t 事件的指针，这样，从
io_getevents 方法获取的
io_event 对象中的
data 也是这个指针
*/
aio-\>aiocb.aio*data = (uint64_t) (uintptr_t) ev; aio-\>aiocb.aio_lio_opcode = IOCB_CMD_PREAD; aio-\>aiocb.aio_fildes = file-\>fd; aio-\>aiocb.aio_buf = (uint64_t) (uintptr_t) buf; aio-\>aiocb.aio_nbytes = size; aio-\>aiocb.aio_offset = offset; aio-\>aiocb.aio_flags = IOCB_FLAG_RESFD; aio-\>aiocb.aio_resfd = ngx_eventfd; /*
ngx*file_aio_event_handler，它的调用关系类似这样
:epoll_wait 中调用
ngx_epoll_eventfd_handler 方法将当前事件放入到
ngx_posted_events 队列中，在延后执行的队列中调用
ngx_file_aio_event_handler 方法
*/
ev-\>handler = ngx*file_aio_event_handler; piocb[0] = &aio-\>aiocb; /*调用
io*submit 向
ngx_aio_ctx 异步
I/O 上下文中添加
1 个事件，返回
1 表示成功
*/
if (io_submit(ngx_aio_ctx, 1, piocb) == 1) {
ev-\>active = 1;
ev-\>ready = 0;
ev-\>complete = 0;
return NGX_AGAIN;
}
…
}
下面看一下 ngx_event_aio_t 结构体的定义。

typedef struct ngx_event_aio_s ngx_event_aio_t; struct ngx_event_aio_s {
void *data;
// 这是真正由业务模块实现的方法，在异步
I/O 事件完成后被调用
ngx_event_handler_pt handler; ngx_file_t *file;
ngx_fd_t fd;
if (NGX_HAVE_EVENTFD)
int64_t res;
else
ngx_err_t err;
size_t nbytes;
endif
if (NGX_HAVE_AIO_SENDFILE)
off_t last_offset;
endif
// 这里的
ngx_aiocb_t 就是
9.9.1 节中介绍的
iocb 结构体
ngx_aiocb_t aiocb;
ngx_event_t event;
};
这样，ngx_file_aio_read 方法会向异步 I/O 上下文中添加事件，该 epoll_wait 在通过
ngx_eventfd 描述符检测到异步 I/O 事件后，会再调用 ngx_epoll_eventfd_handler 方法将 io_event
事件取出来，放入 ngx_posted_events 队列中延后执行。ngx_posted_events 队列中的事件执行
时，则会调用 ngx_file_aio_event_handler 方法。下面看一下 ngx_file_aio_event_handler 方法做了
些什么，代码如下所示。

static void ngx_file_aio_event_handler(ngx_event_t *ev) {
ngx_event_aio_t *aio;
aio = ev-\>data;
aio-\>handler(ev);
}
这里调用了 ngx_event_aio_t 结构体的 handler 回调方法，这个回调方法是由真正的业务模
块实现的，也就是说，任一个业务模块想使用文件异步 I/O，就可以实现 handler 方法，这
样，在文件异步操作完成后，该方法就会被回调。

9.10 TCP 协议与 Nginx
作为 Web 服务器的 nginx，主要任务当然是处理好基于 TCP 的 HTTP 协议，本节将深入 TCP
协议的实现细节（linux 下）以更好地理解 Nginx 事件处理机制。

TCP 是一个面向连接的协议，它必须基于建立好的 TCP 连接来为通信的两方提供可靠的
字节流服务。建立 TCP 连接是我们耳熟能详的三次握手：
1）客户端向服务器发起连接（SYN）。

2）服务器确认收到并向客户端也发起连接（ACK+SYN）。

3）客户端确认收到服务器发起的连接（ACK）。

这个建立连接的过程是在操作系统内核中完成的，而如 Nginx 这样的应用程序只是从内
核中取出已经建立好的 TCP 连接。大多时候，Nginx 是作为连接的服务器方存在的，我们看一
看 Linux 内核是怎样处理 TCP 连接建立的，如图 9-8 所示。

图 9-8 服务器端建立 TCP 连接的简化示意图
图 9-8 中简单地表达了一个观点：内核在我们调用 listen 方法时，就已经为这个监听端口
建立了 SYN 队列和 ACCEPT 队列，当客户端使用 connect 方法向服务器发起 TCP 连接，随后图
中 1.1 步骤客户端的 SYN 包到达了服务器后，内核会把这一信息放到 SYN 队列（即未完成握手
队列）中，同时回一个 SYN+ACK 包给客户端。2.1 步骤中客户端再次发来了针对服务器 SYN
包的 ACK 网络分组时，内核会把连接从 SYN 队列中取出，再把这个连接放到 ACCEPT 队列
（即已完成握手队列）中。而服务器在第 3 步调用 accept 方法建立连接时，其实就是直接从
ACCEPT 队列中取出已经建好的连接而已。

这样，如果大量连接同时到来，而应用程序不能及时地调用 accept 方法，就会导致以上
两个队列满（ACCEPT 队列满，进而也会导致 SYN 队列满），从而导致连接无法建立。这其
实很常见，比如 Nginx 的每个 worker 进程都负责调用 accept 方法，如果一个 Nginx 模块在处理请
求时长时间陷入了某个方法的执行中（如执行计算或者等待 IO），就有可能导致新连接无法
建立。

建立好连接后，TCP 提供了可靠的字节流服务。怎么理解所谓的“可靠”呢？可以简单概
括为以下 4 点：
1）TCP 的 send 方法可以发送任意大的长度，但数据链路层不会允许一个报文太大的，当
报文长度超过 MTU 大小时，它一定会把超大的报文切成小报文。这样的场景是不被 TCP 接受
的，切分报文段既然不可避免，那么就只能发生在 TCP 协议内部，这才是最有效率的。

2）每一个报文在发出后都必须收到“回执”——ACK，确保对方收到，否则会在超时时
间达到后重发。相对的，接收到一个报文时也必须发送一个 ACK 告诉对方。

3）报文在网络中传输时会失序，TCP 接收端需要重新排序失序的报文，组合成发送时
的原序再给到应用程序。当然，重复的报文也要丢弃。

4）当连接的两端处理速度不一致时，为防止 TCP 缓冲区溢出，还要有个流量控制，减
缓速度更快一方的发送速度。

从以上 4 点可以看到，内核为每一个 TCP 连接都分配了内存分别充当发送、接收缓冲
区，这与 Nginx 这种应用程序中的用户态缓存不同。搞清楚内核的 TCP 读写缓存区，对于我们
判断 Nginx 的处理能力很有帮助，毕竟无论内核还是应用程序都在抢物理内存。

先来看看调用 send 这样的方法发送 TCP 字节流时，内核到底做了哪些事。图 9-9 是一个简
单的 send 方法调用时的流程示意图。

TCP 连接建立时，就可以判断出双方的网络间最适宜的、不会被再次切分的报文大小，
TCP 层把它叫做 MSS 最大报文段长度（当然，MSS 是可变的）。在图 9-9 的场景中，假定待发
送的内存将按照 MSS 被切分为 3 个报文，应用程序在第 1 步调用 send 方法、第 10 步 send 方法返
回之间，内核的主要任务是把用户态的内存内容拷贝到内核态的 TCP 缓冲区上，在第 5 步时
假定内核缓存区暂时不足，在超时时间内又等到了足够的空闲空间。从图中可以看到，send
方法成功返回并不等于就把报文发送出去了（当然更不等于对方接收到了报文）。

图 9-9 send 方法执行时的流程示意图
当调用 recv 这样的方法接收报文时，Nginx 是基于事件驱动的，也就是说只有 epoll 通知
worker 进程收到了网络报文，recv 才会被调用（socket 也被设为非阻塞模式）。图 9-10 就是一
个这样的场景，在第 1~4 步表示接收到了无序的报文后，内核是怎样重新排序的。第 5 步开
始，应用程序调用了 recv 方法，内核开始把 TCP 读缓冲区的内容拷贝到应用程序的用户态内
存中，第 13 步 recv 方法返回拷贝的字节数。图中用到了 linux 内核中为 TCP 准备的 2 个队列：
receive 队列是允许用户进程直接读取的，它是将已经接收到的 TCP 报文，去除了 TCP 头部、
排好序放入的、用户进程可以直接按序读取的队列；out_of_order 队列存放乱序的报文。

回过头来看，Nginx 使用好 TCP 协议主要在于如何有效率地使用 CPU 和内存。只在必要时
才调用 TCP 的 send/recv 方法，这样就避免了无谓的 CPU 浪费。例如，只有接收到报文，甚至
只有接收到足够多的报文（SO_RCVLOWAT 阈值），worker 进程才有可能调用 recv 方法。同
样，只在发送缓冲区有空闲空间时才去调用 send 方法。这样的调用才是有效率的。Nginx 对内
存的分配是很节俭的，但 Linux 内核使用的内存又如何控制呢？
图 9-10 recv 方法执行时的流程示意图
首先，我们可以控制内存缓存的上限，例如基于 setsockopt 方法实现的 SO_SNDBUF、
SO_RCVBUF（Nginx 的 listen 配置里的 sndbuf 和 rcvbuf 也是在改它们，参见 2.4.1 节）。

SO_SNDBUF 表示这个连接上的内核写缓存上限（事实上 SO_SNDBUF 也并不是精确的上限，
在内核中会把这个值翻一倍再作为写缓存上限使用）。它受制于系统级配置的上下限
net.core.wmem_max（参见 1.3.4 节）。SO_RCVBUF 同理。读写缓存的实际内存大小与场景有
关。对读缓存来说，接收到一个来自连接对端的 TCP 报文时，会导致读缓存增加，如果超过
了读缓存上限，那么这个报文会被丢弃。当进程调用 read、recv 这样的方法读取字节流时，
读缓存就会减少。因此，读缓存是一个动态变化的、实际用到多少才分配多少的缓冲内存。

当用户进程调用 send 方法发送 TCP 字节流时，就会造成写缓存增大。当然，如果写缓存已经
到达上限，那么写缓存维持不变，向用户进程返回失败。而每当接收到连接对端发来的
ACK，确认了报文的成功发送时，写缓存就会减少。可见缓存上限所起作用为：丢弃新报
文，防止这个 TCP 连接消耗太多的内存。

其次，我们可以使用 Linux 提供的自动内存调整功能。

net.ipv4.tcp_moderate_rcvbuf = 1
默认 tcp_moderate_rcvbuf 配置为 1，表示打开了 TCP 内存自动调整功能。若配置为 0，这
个功能将不会生效（慎用）。

注意当我们在编程中对连接设置了 SO_SNDBUF、SO_RCVBUF，将会使 Linux 内
核不再对这样的连接执行自动调整功能！
那么，这个功能到底是怎样起作用的呢？举个例子，请看下面的缓存上限配置：
net.ipv4.tcp_rmem = 8192 87380 16777216
net.ipv4.tcp_wmem = 8192 65536 16777216
net.ipv4.tcp_mem = 8388608 12582912 16777216
tcp_rmem[3]数组表示任何一个 TCP 连接上的读缓存上限，其中 tcp_rmem[0]表示最小上
限，tcp_rmem[1]表示初始上限（注意，它会覆盖适用于所有协议的 rmem_default 配置），
tcp_rmem[2]表示最大上限。tcp_wmem[3]数组表示写缓存，与 tcp_rmem[3]类似。

tcp_mem[3]数组就用来设定 TCP 内存的整体使用状况，所以它的值很大（它的单位也不
是字节，而是页——4KB 或者 8KB 等这样的单位！）。这 3 个值定义了 TCP 整体内存的无压力
值、压力模式开启阈值、最大使用值。以这 3 个值为标记点则内存共有 4 种情况（如图 9-11 所
示）：
1）当 TCP 整体内存小于 tcp_mem[0]时，表示系统内存总体无压力。若之前内存曾经超过
了 tcp_mem[1]使系统进入内存压力模式，那么此时也会把压力模式关闭。此时，只要 TCP 连
接使用的缓存没有达到上限，那么新内存的分配一定是成功的。

2）当 TCP 内存在 tcp_mem[0]与 tcp_mem[1]之间时，系统可能处于内存压力模式，例如总
内存刚从 tcp_mem[1]之上下来；也可能是在非压力模式下，例如总内存刚从 tcp_mem[0]以下
上来。

此时，无论是否在压力模式下，只要 TCP 连接所用缓存未超过 tcp_rmem[0]或者
tcp_wmem[0]，那么都一定能成功分配新内存。否则，基本上就会面临分配失败的状况。

（还有少量例外场景允许分配内存成功，这里不纠结内核的实现细节，故略过。）
3）当 TCP 内存在 tcp_mem[1]与 tcp_mem[2]之间时，系统一定处于系统压力模式下。行为
与情况 2 相同。

4）当 TCP 内存在 tcp_mem[2]之上时，所有的新 TCP 缓存分配都会失败。

图 9-11 Linux 下 TCP 缓存上限的自适应调整
9.11 小结
本章在具体的事件驱动模块基础上以 epoll 方式为例，完整地阐述了 Nginx 的事件驱动机
制，并介绍了 3 个与事件驱动密切相关的 Nginx 模块，同时说明了事件驱动中的流程是如何执
行的。另外，还介绍了 Nginx 在高并发服务器设计上的一些技巧，这不仅对我们了解 Nginx 的
架构有所帮助，更对我们以后设计独立的高性能服务器有非常大的启发意义。本章内容也是
Nginx 其他模块的基础，之后的章节都是在讨论事件消费模块，特别是后续的 HTTP 模块，在
学习它们的设计方法时我们会经常性地返回到本章的事件驱动机制中。
